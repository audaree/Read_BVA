{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2e3c0d",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Julia program to read a selected .BVA file and display 30-minute time series plots\n",
    "## JW October 2022\n",
    "#using ContinuousWavelets \n",
    "using ContinuousWavelets, CSV\n",
    "using Dates, DataFrames, Distributions, DSP\n",
    "using FFTW\n",
    "##using Gtk\n",
    "using LaTeXStrings\n",
    "using NativeFileDialog\n",
    "using Plots\n",
    "using Printf\n",
    "using Statistics #, StatsPlots\n",
    "#using Suppressor: @suppress\n",
    "using Wavelets\n",
    "\n",
    "##import Pkg; Pkg.add(\"Suppressor\")\n",
    "## See https://github.com/JuliaIO/Suppressor.jl\n",
    "##using Suppressor: @suppress\n",
    "\n",
    "include(\"./read_BVA_processing_tools.jl\")\n",
    "include(\"./read_BVA_plotting_tools.jl\")\n",
    "\n",
    "function handle_gaps(df)\n",
    "################################################    \n",
    "# a function to identify where gaps found in sequence numbers\n",
    "#    where gaps occur a dummy record is inserted into the df\n",
    "#    and both Status1 and Status2 are flagged with a '!' \n",
    "    \n",
    "    nums = []\n",
    "\n",
    "    # Convert sequence number value from Hex to Integer\n",
    "    for i in 1:nrow(df)\n",
    "        str = df.Sequence[i]\n",
    "        push!(nums,parse(Int, str[1], base=16) * 16 + parse(Int, str[2], base=16))\n",
    "    end\n",
    "\n",
    "    # Determine number of gaps in df rows\n",
    "    counter = diff(nums)\n",
    "    gaps = findall(counter.<1)\n",
    "    counter[gaps] .+= 256;\n",
    "\n",
    "    # now find where a counter is > 1 indicating number of gaps in transmission\n",
    "    gaps = findall(counter.>1)\n",
    "    ll = length(gaps)\n",
    "        \n",
    "    if ll > 0\n",
    "        \n",
    "        println(cumsum(gaps),\" found in file!\")\n",
    "        println(\"File contains \",nrow(df),\" records\")\n",
    "    \n",
    "\n",
    "##    df1 = deepcopy(df)\n",
    "\n",
    "        # need to work through the gaps in reverse order in order to preserve row numbers\n",
    "        for i in ll:-1:1\n",
    "            # for each gap, get sequence number of last valid row\n",
    "            sequence_number = df[gaps[i],:].Sequence\n",
    "\n",
    "            # for number of gaps, insert \"FFF\" values into df and '!' into the status indicators\n",
    "            for j in 1:counter[gaps[i]]-1\n",
    "\n",
    "                sequence_number_hex = uppercase(string((parse(Int, sequence_number[1], base=16) * 16 + parse(Int, sequence_number[2], base=16)) + j, base=16, pad=2))\n",
    "        ##        println(i,' ',j,' ',gaps[i]+j,' ',counter[gaps[i]],' ',sequence_number,' ',sequence_number_hex,' ',\"!\",' ',\"FFFFFFFFFFFFFFFFFF\",' ',\"!\",' ',\"FFFFFF\")\n",
    "                insert!(df, gaps[i]+j, [sequence_number_hex, '!', \"FFFFFFFFFFFFFFFFFF\", '!', \"FFFFFF\"])\n",
    "\n",
    "            end\n",
    "                \n",
    "        end\n",
    "\n",
    "        println(\"File now contains \",nrow(df),\" records\")\n",
    "                \n",
    "    else\n",
    "                \n",
    "        println(\"No gaps in record\")\n",
    "                \n",
    "    end\n",
    "\n",
    "    return (df)\n",
    "        \n",
    "    end    # handle_gaps()\n",
    "    \n",
    "\n",
    "function string2hex(str)\n",
    "################################################    \n",
    "    parsed_val = (parse(Int, str[1], base=16) * 16 + parse(Int, str[2], base=16));\n",
    "    if parsed_val == 0\n",
    "        hex = 0x00\n",
    "    else\n",
    "        hex = [(parsed_val>>((i-1)<<3))%UInt8 for i in 1:sizeof(parsed_val)-leading_zeros(parsed_val)>>3][1]\n",
    "    end\n",
    "    \n",
    "    return(hex)\n",
    "    \n",
    "    end    # string2hex()\n",
    "\n",
    "\n",
    "################################################\n",
    "################################################\n",
    "##           START OF MAIN PROGRAM\n",
    "################################################\n",
    "################################################\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "# Select a HVA or BVA file\n",
    "##infil = pick_file(\"C:\\\\QGHL\\\\Wave_data\\\\Bris\\\\BVA\\\\\", filterlist=\"HVA,BVA;hva,bva\");\n",
    "##infil = pick_file(\"C:\\\\QGHL\\\\Wave_data\\\\Brisbane_offshore\\\\\", filterlist=\"HVA,BVA;hva,bva\");\n",
    "infil = pick_file(\"C:\\\\\");\n",
    "println(\"Selected \",infil)\n",
    "flush(stdout)\n",
    "\n",
    "if uppercase(split(infil, \".\")[end]) == \"HVA\"\n",
    "    \n",
    "#    df = DataFrame(CSV.File(infil,header=0, delim=\",-\"));\n",
    "#    rename!(df,[:Sequence,:Data, :Packet]);\n",
    "    \n",
    "    # read data from file to df\n",
    "    df = DataFrame(CSV.File(infil,header=0))\n",
    "\n",
    "    # add column names to df as shown in DWTP Section 2.1 pp 18-19 - Status1; Data; Status2; Packet\n",
    "    transform!(df, :Column2 => ByRow(x -> x[1]) => :Status1)\n",
    "    transform!(df, :Column2 => ByRow(x -> x[2:19]) => :Data)\n",
    "    transform!(df, :Column3 => ByRow(x -> x[1]) => :Status2)\n",
    "    transform!(df, :Column3 => ByRow(x -> x[2:7]) => :Packet)\n",
    "\n",
    "    rename!(df,:Column1 => :Sequence)\n",
    "    DataFrames.select!(df, Not([:Column2, :Column3]))\n",
    "            \n",
    "    df1 = deepcopy(df)\n",
    "    df = handle_gaps(df)    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "    # identify rows where channel not received OK ( see DWTP 2.1 pp. 18-19)\n",
    "    bad = findall(df[!,:Status1] .== '!')\n",
    "    if !isempty(bad)\n",
    "        println(length(bad),\" unrecoverable records found!\")\n",
    "    else\n",
    "        println(\"No unrecoverable records found!\")\n",
    "    end \n",
    "\n",
    "###    delete!(df, bad) # drop rows where channel is damaged beyond repair\n",
    "    suspect = findall(df[!,:Status1] .== '=') # flag rows where channel is damaged but has been repaired successfully\n",
    "    if !isempty(suspect)\n",
    "        println(length(suspect),\" damaged records found that have been repaired successfully!\")\n",
    "    else\n",
    "        println(\"All records received OK!\")\n",
    "    end \n",
    "\n",
    "    # create a Packet vector of hex values\n",
    "    packet = []\n",
    "    \n",
    "    for i in 1:length(df.Packet)\n",
    "        push!(packet,string2hex(SubString(df.Packet[i],1,2)))\n",
    "        push!(packet,string2hex(SubString(df.Packet[i],3,4)))\n",
    "        push!(packet,string2hex(SubString(df.Packet[i],5,6)))\n",
    "    end  \n",
    "    \n",
    "    # create a Data vector of hex values\n",
    "    Data = lowercase.(df.Data)\n",
    "    \n",
    "elseif uppercase(split(infil, \".\")[end]) == \"BVA\"\n",
    "    \n",
    "    #Change the type-interpretation of the binary file data to unsigned integer\n",
    "    println(\"Reading BINARY data from \",infil)\n",
    "    data = reinterpret(UInt8, read(infil));\n",
    "\n",
    "    # turn the data vector into a matrix of 12 values matching hexadecimal bytes - see DWTP 2.1 p.18\n",
    "    cols = 12\n",
    "    rows = Int(length(data) / cols)\n",
    "    mat = reshape(view(data, :), cols, :);\n",
    "\n",
    "    # Interleave last 4 matrix columns to form packet vector\n",
    "    ## based on mschauer @ https://discourse.julialang.org/t/combining-two-arrays-with-alternating-elements/15498/2\n",
    "    packet = collect(Iterators.flatten(zip(mat[10,:],mat[11,:],mat[12,:])));\n",
    "    \n",
    "    ## get data for the Heave, North, and West displacements\n",
    "    Data = []\n",
    "\n",
    "    # Convert binary data to hexidecimal vectors\n",
    "    j = 0\n",
    "    println(\"Building displacements vectors - this takes a while!\")\n",
    "    while true\n",
    "\n",
    "        try\n",
    "            aa = []\n",
    "\n",
    "            for i = j*12+1:j*12+12\n",
    "                push!(aa,string(data[i], base = 16, pad = 2))\n",
    "            end\n",
    "\n",
    "            push!(Data,join(aa)[1:18])\n",
    "\n",
    "        catch\n",
    "\n",
    "            # escape if something is amiss        \n",
    "            break\n",
    "\n",
    "        end\n",
    "        j = j+1\n",
    "\n",
    "    end\n",
    "\n",
    "else\n",
    "    println(\"Not able to read this file type at present\")\n",
    "    exit()\n",
    "end\n",
    "\n",
    "# find all occurrences of 0x7e in packet vector\n",
    "aa = findall(x->x.==0x7e, vec(packet));\n",
    "\n",
    "# Create the df's to hold the processed data and setup their column structure\n",
    "f20_vals = []; f21_vals = []; f23_vals = []; f25_vals = []; f26_vals = []; f28_vals = []; f29_vals = [];\n",
    "    f80_vals = []; f81_vals = []; f82_vals = []; fc1_vals = []; fc3_vals = []\n",
    "\n",
    "f20_df = DataFrame(Date = [], Segments = [], Smax = [])\n",
    "for i in 0:99\n",
    "    col_name = \"S$i\"\n",
    "    f20_df[!,col_name] = []\n",
    "end\n",
    "f21_df = DataFrame(Date = [], Segments = [])\n",
    "\n",
    "for i in 0:99\n",
    "    col_name = \"Dir$i\"\n",
    "    f21_df[!,col_name] = []\n",
    "end\n",
    "for i in 0:99\n",
    "    col_name = \"Spread$i\"\n",
    "    f21_df[!,col_name] = []\n",
    "end\n",
    "f23_df = DataFrame(Date = [], Segments = [], Match_vector = [], Sample_number = [])\n",
    "f25_df = DataFrame(Date = [], Segments = [], Hs = [], Ti = [], Te = [], T1 = [], Tz = [], T3 = [], \n",
    "    Tc = [], Rp = [], Tp = [], Smax = [], Theta_p = [], σ_p = [])\n",
    "f26_df = DataFrame(Date = [], Hmax = [], Thmax = [], Tmax = [], Htmax = [], Havg = [], Tavg = [], \n",
    "    Hsrms = [], Nw = [], Nc = [], Epsilon = [], Coverage = [])\n",
    "f28_df = DataFrame(Date = [], Segments = [])\n",
    "for i in 0:99\n",
    "    col_name = \"m2_$i\"\n",
    "    f28_df[!,col_name] = []\n",
    "end\n",
    "for i in 0:99\n",
    "    col_name = \"n2_$i\"\n",
    "    f28_df[!,col_name] = []\n",
    "end\n",
    "for i in 0:99\n",
    "    col_name = \"k$i\"\n",
    "    f28_df[!,col_name] = []\n",
    "end\n",
    "f29_df = DataFrame(Date = [], Coverage = [], Nw = [], Epsilon = [], Hmax = [], THmax = [], H10 = [], \n",
    "    TH10 = [], H3 = [], TH3 = [], Havg = [], Tavg = [])\n",
    "for i in 0:22\n",
    "    col_name = \"Hq$i\"\n",
    "    f29_df[!,col_name] = []\n",
    "end\n",
    "f80_df = DataFrame(Date = [], Latitude = [], Longitude = [])\n",
    "f81_df = DataFrame(Date = [], SST = [])\n",
    "f82_df = DataFrame(Date = [], Firmware_Version = [], Speed = [], Direction = [], SST = [])\n",
    "fc1_df = DataFrame(Date = [], Firmware = [], Hatch_uid = [], Hull_uid = [], Uptime = [], \n",
    "    Battery_energy = [], Boostcaps_energy = [],\n",
    "    Hatch_temp = [], Battery_voltage = [], Batteries_per_section = [], Battery_section_number = [], \n",
    "    Initial_battery_energy = [], Ov = [], Cv = [], Ox = [], Oy = [], Cx = [], Cy = [], μ₀ = [], \n",
    "    σ₀ = [], μᵢ = [], σᵢ = [], μₕ = [], σh = [], Cpitch = [], Croll = [], Tensor = [])\n",
    "fc3_df = DataFrame(Date = [], Battery_life = [])\n",
    "\n",
    "# determine number of records\n",
    "max_val = length(aa)-1\n",
    "\n",
    "# Decode the packet data to messages\n",
    "# refer to Section 2.1.2 Decoding the packet data to messages p. 20\n",
    "messages = []\n",
    "\n",
    "println(\"Processing the Packet vectors\")\n",
    "for i in 1:max_val\n",
    "    # determine packet length\n",
    "    first = aa[i]+1\n",
    "    last = aa[i+1]\n",
    "            \n",
    "    list1 = []; list2 = []\n",
    "    \n",
    "    if (last-first > 1)\n",
    "        global decoded = []\n",
    "        decoded = packet[first:last-1]\n",
    "        decode_length = length(decoded)\n",
    "                \n",
    "        bb = findall(x->x.==0x7d, vec(decoded));\n",
    "            \n",
    "        if bb != []\n",
    "\n",
    "            # do an xor of elements with 0x7d\n",
    "            for ii in bb\n",
    "                if ii<(last-first)\n",
    "                    decoded[ii+1] = decoded[ii+1] ⊻ 0x20 # set the xor value as 0x20 vide 2.1.2 p.20\n",
    "                end\n",
    "            end\n",
    "\n",
    "            # remove the 0x7d\n",
    "            deleteat!(decoded::Vector, bb)\n",
    "            decode_length = length(decoded)\n",
    "\n",
    "        end\n",
    "\n",
    "##        println(string(decoded[2], base=16, pad=2),' ',length(decoded))\n",
    "\n",
    "        # look for vectors of the spectrum synchronisation message (0xF23)\n",
    "        if decoded[2] == 0x20\n",
    "                    \n",
    "            if decode_length !=161\n",
    "                println(\"Alert 0xF20 message length is \",decode_length,\" but should be 161\")\n",
    "            else           \n",
    "                heave_spectrum = []\n",
    "                append!(f20_vals,decoded)\n",
    "                timestamp,segments,smax,heave_spectrum = process_f20(decoded,heave_spectrum)         \n",
    "                list_1 = [timestamp,segments,smax]\n",
    "                push!(f20_df, [list_1; heave_spectrum])\n",
    "            end\n",
    "            \n",
    "        elseif decoded[2] == 0x21\n",
    "                    \n",
    "            if decode_length !=309\n",
    "                println(\"Alert 0xF21 message length is \",decode_length,\" but should be 309\")\n",
    "            else\n",
    "\n",
    "                global direction = []\n",
    "                global spread = []\n",
    "                append!(f21_vals,decoded)\n",
    "                timestamp,segments,direction,spread = process_f21(decoded,direction,spread)\n",
    "\n",
    "                list1 = [timestamp,segments]\n",
    "                list2 = [direction; spread]\n",
    "\n",
    "                push!(f21_df, [list1; list2])\n",
    "\n",
    "            end\n",
    "                    \n",
    "        elseif decoded[2] == 0x23\n",
    "                  \n",
    "            if decode_length !=22\n",
    "                println(\"Alert 0xF23 message length is \",decode_length,\" but should be 22\")\n",
    "            else\n",
    "\n",
    "                append!(f23_vals,decoded)\n",
    "                timestamp,segments_used,match_vector,sample_number = process_f23(decoded)\n",
    "                push!(f23_df, [timestamp,segments_used,match_vector,sample_number])\n",
    "\n",
    "            end\n",
    "\n",
    "        elseif decoded[2] == 0x25\n",
    "\n",
    "            if decode_length !=27\n",
    "                println(\"Alert 0xF25 message length is \",decode_length,\" but should be 27\")\n",
    "            else\n",
    "\n",
    "                append!(f25_vals,decoded)\n",
    "                timestamp,segments,hs,ti,te,t1,tz,t3,tc,rp,tp,smax,theta_p,σ_p = process_f25(decoded)\n",
    "                push!(f25_df, [timestamp,segments,hs,ti,te,t1,tz,t3,tc,rp,tp,smax,theta_p,σ_p])\n",
    "            end\n",
    "\n",
    "            elseif decoded[2] == 0x26\n",
    "\n",
    "            if decode_length !=25\n",
    "                println(\"Alert 0xF26 message length is \",decode_length,\" but should be 25\")\n",
    "            else                    \n",
    "\n",
    "                append!(f26_vals,decoded)\n",
    "                timestamp,hmax,thmax,tmax,htmax,havg,tavg,hsrms,nw,nc,epsilon,coverage = process_f26(decoded)\n",
    "                push!(f26_df, [timestamp,hmax,thmax,tmax,htmax,havg,tavg,hsrms,nw,nc,epsilon,coverage])\n",
    "            end\n",
    "\n",
    "        elseif decoded[2] == 0x28\n",
    "\n",
    "            if decode_length !=459\n",
    "                println(\"Alert 0xF28 message length is \",decode_length,\" but should be 459\")\n",
    "            else                    \n",
    "            \n",
    "                m2 = []\n",
    "                n2 = []\n",
    "                k = []\n",
    "                append!(f28_vals,decoded)\n",
    "                timestamp,segments,m2,n2,k = process_f28(decoded,m2,n2,k)\n",
    "\n",
    "                global list1 = [timestamp,segments]\n",
    "                global list2 = [m2; n2; k]\n",
    "\n",
    "                push!(f28_df, [list1; list2])\n",
    "            end\n",
    "            \n",
    "        elseif decoded[2] == 0x29\n",
    "\n",
    "            if decode_length !=59\n",
    "                println(\"Alert 0xF29 message length is \",decode_length,\" but should be 59\")\n",
    "            else                 \n",
    "                    \n",
    "                hq = []\n",
    "                append!(f29_vals,decoded)\n",
    "                timestamp,coverage,nw,epsilon,hmax,thmax,h10,th10,h3,th3,havg,tavg,hq = process_f29(decoded,hq)         \n",
    "                list_1 = [timestamp,coverage,nw,epsilon,hmax,thmax,h10,th10,h3,th3,havg,tavg]\n",
    "                push!(f29_df, [list_1; hq])\n",
    "            end\n",
    "        elseif decoded[2] == 0x80\n",
    "\n",
    "            if decode_length !=14\n",
    "                println(\"Alert 0xF80 message length is \",decode_length,\" but should be 14\")\n",
    "            else                 \n",
    "            \n",
    "                append!(f80_vals,decoded)\n",
    "                timestamp,latitude,longitude = process_f80(decoded)\n",
    "                push!(f80_df, [timestamp,latitude,longitude])\n",
    "            end         \n",
    "        elseif decoded[2] == 0x81\n",
    "\n",
    "            if decode_length !=10\n",
    "                println(\"Alert 0xF81 message length is \",decode_length,\" but should be 10\")\n",
    "            else                 \n",
    "            \n",
    "                append!(f81_vals,decoded)\n",
    "                timestamp,sst = process_f81(decoded)\n",
    "                push!(f81_df, [timestamp,sst])\n",
    "            end\n",
    "        elseif decoded[2] == 0x82\n",
    "\n",
    "            if decode_length !=29\n",
    "                println(\"Alert 0xF82 message length is \",decode_length,\" but should be 29\")\n",
    "            else                 \n",
    "            \n",
    "                append!(f82_vals,decoded)\n",
    "                timestamp,firmware_version,speed,direction,sst = process_f82(decoded)\n",
    "                push!(f82_df, [timestamp,firmware_version,speed,direction,sst])\n",
    "\n",
    "            end\n",
    "        elseif decoded[2] == 0xc1\n",
    "\n",
    "            if decode_length !=67\n",
    "                println(\"Alert 0xFC1 message length is \",decode_length,\" but should be 67\")\n",
    "            else                 \n",
    "            \n",
    "                append!(fc1_vals,decoded)\n",
    "                timestamp,firmware,hatch_uid,hull_uid,uptime,battery_energy,boostcaps_energy,hatch_temp,battery_voltage,batteries_per_section,\n",
    "                    battery_section_number,initial_battery_energy,ov,cv,ox,oy,cx,cy,μ₀,σ₀,μᵢ,σᵢ,μₕ,σh,cpitch,croll,tensor = process_fc1(decoded)   \n",
    "\n",
    "                push!(fc1_df, [timestamp,firmware,hatch_uid,hull_uid,uptime,battery_energy,boostcaps_energy,hatch_temp,battery_voltage,batteries_per_section,\n",
    "                    battery_section_number,initial_battery_energy,ov,cv,ox,oy,cx,cy,μ₀,σ₀,μᵢ,σᵢ,μₕ,σh,cpitch,croll,tensor])\n",
    "            end\n",
    "        elseif decoded[2] == 0xc3\n",
    "\n",
    "            if decode_length !=9\n",
    "                println(\"Alert 0xFC3 message length is \",decode_length,\" but should be 9\")\n",
    "            else                 \n",
    "            \n",
    "                append!(fc3_vals,decoded)\n",
    "                timestamp,ble = process_fc3(decoded)\n",
    "                push!(fc3_df, [timestamp,ble])\n",
    "\n",
    "            end\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "end\n",
    "    \n",
    "# remove duplicates from dataframes\n",
    "f20_df = unique(f20_df)\n",
    "f21_df = unique(f21_df)\n",
    "f23_df = unique(f23_df)\n",
    "f25_df = unique(f25_df)\n",
    "f26_df = unique(f26_df)    \n",
    "f28_df = unique(f28_df)\n",
    "f29_df = unique(f29_df)\n",
    "f80_df = unique(f80_df)\n",
    "f81_df = unique(f81_df)\n",
    "f82_df = unique(f82_df)\n",
    "fc1_df = unique(fc1_df)\n",
    "fc3_df = unique(fc3_df)\n",
    "\n",
    "println(\"All file data read!\")\n",
    "println(\"Preparing to plot data\")\n",
    "flush(stdout)\n",
    "    \n",
    "# remove those vectors from F23 df that are not located in the Data vector df\n",
    "f23_df[!,\"Data_vector\"] = [findfirst(x->x==i, Data) for i in f23_df.Match_vector];\n",
    "\n",
    "# Do time-series plot of available data\n",
    "plot_f29(f29_df)\n",
    "\n",
    "# Plot current speed and direction\n",
    "##plot_f82(f82_df)\n",
    "\n",
    "println(\"Select date from menu for more plots\")\n",
    "\n",
    "################################################\n",
    "################################################\n",
    "##           END OF MAIN PROGRAM\n",
    "################################################\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3229f1-5518-45a3-96e9-dcba33ea331b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using ContinuousWavelets, CSV\n",
    "using Dates, DataFrames, Distributions, DSP\n",
    "using FFTW\n",
    "##using Gtk\n",
    "using LaTeXStrings\n",
    "using NativeFileDialog\n",
    "using Plots\n",
    "using Printf\n",
    "using Statistics #, StatsPlots\n",
    "#using Suppressor: @suppress\n",
    "using Wavelets\n",
    "\n",
    "##import Pkg; Pkg.add(\"Suppressor\")\n",
    "## See https://github.com/JuliaIO/Suppressor.jl\n",
    "##using Suppressor: @suppress\n",
    "\n",
    "include(\"./read_BVA_processing_tools.jl\")\n",
    "include(\"./read_BVA_plotting_tools.jl\")\n",
    "\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "# Select a HVA or BVA file\n",
    "##infil = pick_file(\"C:\\\\QGHL\\\\Wave_data\\\\Bris\\\\BVA\\\\\", filterlist=\"HVA,BVA;hva,bva\");\n",
    "##infil = pick_file(\"C:\\\\QGHL\\\\Wave_data\\\\Brisbane_offshore\\\\\", filterlist=\"HVA,BVA;hva,bva\");\n",
    "infil = pick_file(\"C:\\\\\");\n",
    "println(\"Selected \",infil)\n",
    "flush(stdout)\n",
    "\n",
    "if uppercase(split(infil, \".\")[end]) == \"HVA\"\n",
    "    \n",
    "#    df = DataFrame(CSV.File(infil,header=0, delim=\",-\"));\n",
    "#    rename!(df,[:Sequence,:Data, :Packet]);\n",
    "    \n",
    "    # read data from file to df\n",
    "    df = DataFrame(CSV.File(infil,header=0))\n",
    "\n",
    "    # add column names to df as shown in DWTP Section 2.1 pp 18-19 - Status1; Data; Status2; Packet\n",
    "    transform!(df, :Column2 => ByRow(x -> x[1]) => :Status1)\n",
    "    transform!(df, :Column2 => ByRow(x -> x[2:19]) => :Data)\n",
    "    transform!(df, :Column3 => ByRow(x -> x[1]) => :Status2)\n",
    "    transform!(df, :Column3 => ByRow(x -> x[2:7]) => :Packet)\n",
    "\n",
    "    rename!(df,:Column1 => :Sequence)\n",
    "    DataFrames.select!(df, Not([:Column2, :Column3]))\n",
    "            \n",
    "    df1 = deepcopy(df)\n",
    "    df = handle_gaps(df)    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "    # identify rows where channel not received OK ( see DWTP 2.1 pp. 18-19)\n",
    "    bad = findall(df[!,:Status1] .== '!')\n",
    "    if !isempty(bad)\n",
    "        println(length(bad),\" unrecoverable records found!\")\n",
    "    else\n",
    "        println(\"No unrecoverable records found!\")\n",
    "    end \n",
    "\n",
    "###    delete!(df, bad) # drop rows where channel is damaged beyond repair\n",
    "    suspect = findall(df[!,:Status1] .== '=') # flag rows where channel is damaged but has been repaired successfully\n",
    "    if !isempty(suspect)\n",
    "        println(length(suspect),\" damaged records found that have been repaired successfully!\")\n",
    "    else\n",
    "        println(\"All records received OK!\")\n",
    "    end \n",
    "\n",
    "    # create a Packet vector of hex values\n",
    "    packet = []\n",
    "    \n",
    "    for i in 1:length(df.Packet)\n",
    "        push!(packet,string2hex(SubString(df.Packet[i],1,2)))\n",
    "        push!(packet,string2hex(SubString(df.Packet[i],3,4)))\n",
    "        push!(packet,string2hex(SubString(df.Packet[i],5,6)))\n",
    "    end  \n",
    "    \n",
    "    # create a Data vector of hex values\n",
    "    Data = lowercase.(df.Data)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442af1a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Locate bad WSE data\n",
    "bad_wse = findall(df.Status1.=='!')\n",
    "\n",
    "# Locate bad Packet data\n",
    "bad_packet = findall(df.Status2.=='!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a35ec",
   "metadata": {},
   "source": [
    "## Do Lagrangian plot of currents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd47de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##import Pkg; Pkg.add(\"Haversine\")\n",
    "\n",
    "using Haversine\n",
    "# see https://juliahub.com/ui/Packages/Haversine/1CvkY/1.1.0\n",
    "\n",
    "# find closest Timestamp in 0xF82 to 0xF80 and use the λ and ϕ of this point as origin\n",
    "start_point = findmin(abs.(f82_df.Date.-f80_df.Date[1]))[2]\n",
    "\n",
    "latitudes = []; longitudes = []\n",
    "\n",
    "p = GeoLocation(λ=f80_df.Longitude[start_point], ϕ= f80_df.Latitude[start_point])\n",
    "\n",
    "push!(longitudes, p.λ)\n",
    "push!(latitudes, p.ϕ)\n",
    "\n",
    "for i in 2:nrow(f82_df)\n",
    "    \n",
    "    θ = f82_df.Direction[i-1]\n",
    "    d = f82_df.Speed[i-1] * 600\n",
    "\n",
    "    p = HaversineDestination(p, θ, d)\n",
    "    \n",
    "    push!(longitudes, p.λ)\n",
    "    push!(latitudes, p.ϕ)\n",
    "    \n",
    "    p = GeoLocation(λ=p.λ, ϕ= p.ϕ)\n",
    "    \n",
    "end\n",
    "\n",
    "f82_df.Lat = latitudes\n",
    "f82_df.Long = longitudes;\n",
    "\n",
    "# remove bad values\n",
    "f82_df = filter(:Lat => x -> !any(f -> f(x), (ismissing, isnothing, isnan)), f82_df)\n",
    "\n",
    "d2 = scatter(f82_df.Long, f82_df.Lat, marker=:circle, ms=:1, msw=:0, label=\"\", xlabel=\"Longitude (°)\", ylabel=\"Latitude (°)\", \n",
    "    leftmargin = 20Plots.mm, bottommargin = 20Plots.mm,\n",
    "    size=(800,800), aspect_ratio=:equal, framestyle = :box, title=\"Lagrangian current plot\")\n",
    "d2 = plot!(f82_df.Long, f82_df.Lat, lc=:lightblue, lw=:0.5, label=\"\", fg_legend=:transparent, bg_legend=:transparent, foreground_color_grid=\"grey\")\n",
    "d2 = scatter!([first(f82_df.Long)],[first(f82_df.Lat)], marker=:utriangle, ms=:5, mc=:green, label=\"First \"*Dates.format(first(f82_df.Date), \"yyyy-mm-dd HH:MM:SS\")*\"\\n\")\n",
    "d2 = scatter!([last(f82_df.Long)],[last(f82_df.Lat)], marker=:dtriangle, ms=:5, mc=:red, label=\"Last \"*Dates.format(last(f82_df.Date), \"yyyy-mm-dd HH:MM:SS\"))\n",
    "\n",
    "ps = plot(d2)\n",
    "\n",
    "display(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d866c4d",
   "metadata": {},
   "source": [
    "# Select individual records to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ca7de",
   "metadata": {},
   "source": [
    "## Select individual wave records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_scaleogram(heave, start_date)\n",
    "################################################\n",
    "    n=length(heave);\n",
    "    t = range(1,n/2.56,length=n);\n",
    "    \n",
    "    # time stamp each WSE\n",
    "    points = collect(0:1:length(heave)-1)/2.56\n",
    "    times = []\n",
    "\n",
    "    for i in 1:length(points)\n",
    "        push!(times,unix2datetime(datetime2unix(start_date) + points[i]))\n",
    "    end\n",
    "\n",
    "    c = wavelet(Morlet(8), β=0.75);\n",
    "    res = ContinuousWavelets.cwt(heave, c)\n",
    "\n",
    "    freqs = getMeanFreq(ContinuousWavelets.computeWavelets(n, c)[1])\n",
    "#    freqs[1] = 0\n",
    "\n",
    "    # display plots to screen\n",
    "    tm_tick = range(first(times),last(times),step=Minute(5))\n",
    "    ticks = Dates.format.(tm_tick,\"HH:MM:SS\")\n",
    "\n",
    "    p1 = heatmap(times, ((freqs.-minimum(freqs))./maximum(freqs)).*0.64, abs.(res)', c=cgrad(:Spectral, rev=true))                \n",
    "\n",
    "    for i in 0:0.1:1.28\n",
    "        hline!(p1, [i], lw=0.5, c=:white, label=\"\")\n",
    "    end\n",
    "\n",
    "    start_date = first(times)\n",
    "    last_date = last(times)\n",
    "\n",
    "    for i in start_date:Minute(5):last_date\n",
    "        vline!(p1, [i], lw=0.5, c=:white, label=\"\")\n",
    "    end\n",
    "\n",
    "    # Plot spectrogram over scalogram\n",
    "    nw=128;\n",
    "    spec = DSP.Periodograms.spectrogram(heave, nw, 120; fs=2.56,window=hanning);\n",
    "\n",
    "    # display plots to screen\n",
    "    tm_tick = range(first(times),last(times),step=Minute(5))\n",
    "    ticks = Dates.format.(tm_tick,\"HH:MM:SS\")\n",
    "\n",
    "    p1 = plot!(first(times) + Microsecond.(ceil.((spec.time) * 1000000)), spec.freq, DSP.Periodograms.power(spec), lw=1, c=cgrad(:Spectral, rev=true), colorbar=false) \n",
    "\n",
    "    plot_wavelet = plot(p1, \n",
    "        xlabel=\"Time\", xlim=(start_date,last_date), xticks=(tm_tick,ticks), xtickfontsize=7,\n",
    "        ylabel=\"Frequency (Hz)\", ylim=(0,0.4), ytickfontsize=8, \n",
    "        title=\"Scaleogram \" * Dates.format(start_date, \"dd/mm/yyyy HH:MM\"), framestyle = :box,\n",
    "        leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(1500, 500), colorbar=false, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)\n",
    "    \n",
    "    # display plots to screen\n",
    "    display(plot_wavelet)\n",
    "    \n",
    "    return()\n",
    "    \n",
    "    end    # plot_scaleogram()\n",
    "\n",
    "using Gtk\n",
    "\n",
    "## Plot 30-minute records\n",
    "# create a vector of dates from the F23 df\n",
    "vector = Dates.format.(f23_df.Date, \"yyyy-mm-dd HH:MM:SS\");\n",
    "\n",
    "## Allow user to get a 30-minute record and do plots\n",
    "cb = GtkComboBoxText()\n",
    "choices = vector\n",
    "\n",
    "for choice in choices\n",
    "    push!(cb,choice)\n",
    "end\n",
    "\n",
    "set_gtk_property!(cb,:active,1)\n",
    "\n",
    "signal_connect(cb, \"changed\") do widget, others...\n",
    "\n",
    "    # get the active index\n",
    "    idx = get_gtk_property(cb, \"active\", Int) + 2\n",
    "    println(idx)\n",
    "  \n",
    "    # get the active string \n",
    "    str = Gtk.bytestring( GAccessor.active_text(cb) ) \n",
    "    \n",
    "    plot_hnw(f23_df,fc1_df,Data,idx)\n",
    "##    plot_spectra(f23_df,f20_df,Data,idx)\n",
    "##    plot_2d(f23_df,Data,idx)\n",
    "##    plot_hnw_2d(f23_df,Data,idx)\n",
    "##    plot_3d(f23_df,Data,idx)\n",
    "##    plot_heave_histogram(f23_df,Data,idx)\n",
    "#    plot_wavelet(f23_df,Data,idx)\n",
    "        \n",
    "    global start_date, start_val, end_val = get_start_end_dates(f23_df,idx)\n",
    "\n",
    "    # get WSEs for desired 30-minute record\n",
    "    global heave, north, west = get_hnw(Data,start_val,end_val)\n",
    "##    plot_scaleogram(heave, start_date)\n",
    "\n",
    "end\n",
    "\n",
    "win = GtkWindow(\"Select Date\",200,200);\n",
    "Gtk.GAccessor.position(win, Gtk.GtkWindowPosition.CENTER);\n",
    "push!(win, cb);\n",
    "showall(win);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63700d59-da42-4b16-854d-ae01d776b044",
   "metadata": {},
   "source": [
    "### Investigate confidence limits for outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3615f714-463e-4f91-b3de-3632237ef036",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Statistics, Plots, Dates\n",
    "\n",
    "# Function to calculate confidence limits\n",
    "function calc_confidence_limits(data, confidence_interval)\n",
    "####################################################################################\n",
    "    \n",
    "    mean_val = mean(data)\n",
    "    std_dev = std(data)\n",
    "    upper_limit = mean_val + confidence_interval * std_dev\n",
    "    lower_limit = mean_val - confidence_interval * std_dev\n",
    "\n",
    "    return(lower_limit, upper_limit)\n",
    "    \n",
    "end    # calc_confidence_limits()\n",
    "\n",
    "\n",
    "# Function to calculate outliers based on IQR\n",
    "function find_outliers_iqr(data)\n",
    "#################################################\n",
    "    \n",
    "    # Calculate the first and third quartiles\n",
    "    Q1 = quantile(data, 0.25)\n",
    "    Q3 = quantile(data, 0.75)\n",
    "\n",
    "    # Calculate the IQR\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Calculate the lower and upper bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify the indices of outliers\n",
    "    outlier_indices = findall(x -> x < lower_bound || x > upper_bound, data)\n",
    "\n",
    "    return(outlier_indices, lower_bound, upper_bound)\n",
    "    \n",
    "end    # find_outliers_iqr()\n",
    "\n",
    "\n",
    "# Function to plot data with outliers and confidence intervals\n",
    "function plot_outliers(data, times, limits_95, limits_99, title_str, ylabel_str, start_minute, end_minute)\n",
    "################################################################################################\n",
    "    \n",
    "    # Apply the IQR method to the data\n",
    "    outlier_indices, lower_bound, upper_bound = find_outliers_iqr(data)\n",
    "\n",
    "    # Initialize the plot\n",
    "    tm_tick = range(times[1], times[end], step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick,\"MM\")\n",
    "##    start_minute = 0\n",
    "##    end_minute = 30\n",
    "\n",
    "    p1 = plot(\n",
    "        xlabel=\"Record Time (Minutes)\", xlabelfontsize=10, ylabel=ylabel_str, ylabelfontsize=10,\n",
    "        title=title_str, titlefontsize=:10,\n",
    "        xlims=(times[1] + Minute(start_minute), times[1] + Minute(end_minute)),\n",
    "        xticks=(tm_tick, ticks), framestyle=:box, size=(1200, 400), ylims=(0,Inf),\n",
    "        fg_legend=:transparent, bg_legend=:transparent,\n",
    "        legend=:topright, leftmargin = 10Plots.mm, bottommargin = 10Plots.mm\n",
    "    )\n",
    "\n",
    "    # Count the number of values exceeding the 99% confidence limit\n",
    "    num_outliers_99 = count(x -> x > limits_99[1], data)\n",
    "    suspect_string = string(\"  \",num_outliers_99, \" values exceeding 99% confidence limit!\")\n",
    "\n",
    "    # Loop through each data point to plot with varying bar widths and color based on outliers\n",
    "    for i in 1:length(data)\n",
    "        # Set color: red if value exceeds the 99% confidence limit, otherwise blue\n",
    "        bar_color = data[i] > limits_99[1] ? :red : :blue\n",
    "        bar_width = data[i] > limits_99[1] ? :3 : :1\n",
    "        bar_alpha = data[i] > limits_99[1] ? :1 : :0.5\n",
    "        \n",
    "\n",
    "        # Add a single bar for each data point with custom width and color\n",
    "        bar!(p1, [times[i]], [data[i]], lc=bar_color, lw=bar_width, fillcolor=bar_color, alpha=bar_alpha, label=false)\n",
    "    end\n",
    "\n",
    "    outlier_indices, mod_z_scores = modified_z_score(data, 3)\n",
    "    # Check if there are outliers and plot them if any are positive\n",
    "    if !isempty(outlier_indices) && any(x -> x > 0, mod_z_scores[outlier_indices])\n",
    "        scatter!(p1, times[outlier_indices], data[outlier_indices], label=\"Possible Outliers\", marker=:circle, color=:red)\n",
    "    end\n",
    "    \n",
    "    # Add horizontal lines for 95% and 99% confidence limits\n",
    "    hline!(p1, [limits_99[1]], color=:red, lw=:2, linestyle=:dash, label=\"99% Confidence Upper Limit\\n\")\n",
    "    hline!(p1, [limits_95[1]], color=:green, lw=:2, linestyle=:dot, label=\"95% Confidence Upper Limit\")\n",
    "\n",
    "    # Annotate plot with the number of outliers\n",
    "    annotate!(times[1] + Minute(start_minute), maximum(data) * 0.9, text(suspect_string, :left, 10))\n",
    "\n",
    "    # Display the plot\n",
    "    display(p1)\n",
    "\n",
    "    return(outlier_indices)\n",
    "    \n",
    "end    # plot_outliers()\n",
    "\n",
    "\n",
    "function find_exact_zero_up_crossings_with_threshold(times, elevations, threshold_mm)\n",
    "#####################################################################################\n",
    "    \n",
    "    threshold = threshold_mm / 1000.0  # Convert mm to meters\n",
    "    crossings = []\n",
    "    \n",
    "    for i in 2:length(elevations)\n",
    "        if elevations[i-1] < -threshold && elevations[i] > threshold\n",
    "            # Linear interpolation to find the exact zero-crossing\n",
    "            t1, t2 = times[i-1], times[i]\n",
    "            e1, e2 = elevations[i-1], elevations[i]\n",
    "            fraction = (threshold - e1) / (e2 - e1)\n",
    "            zero_crossing_time = t1 + Millisecond(round(Int, fraction * (t2 - t1).value))\n",
    "            push!(crossings, zero_crossing_time)\n",
    "\n",
    "            # Check for zero crossing with linear interpolation between negative and positive elevations\n",
    "        elseif elevations[i-1] < -threshold && elevations[i] == 0 # threshold\n",
    "            t1, t2 = times[i-1], times[i]\n",
    "            e1, e2 = elevations[i-1], elevations[i]\n",
    "            fraction = (threshold - e1) / (e2 - e1)\n",
    "            zero_crossing_time = t1 + Millisecond(round(Int, fraction * (t2 - t1).value))\n",
    "            push!(crossings, zero_crossing_time)\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return(crossings)\n",
    "    \n",
    "end    # find_exact_zero_up_crossings_with_threshold()\n",
    "    \n",
    "\n",
    "function calculate_valid_wave_heights_periods(elevations, zero_crossings, times, threshold_mm)\n",
    "##############################################################################################\n",
    "    \n",
    "    threshold = threshold_mm / 1000.0  # Convert mm to meters\n",
    "    wave_heights = []\n",
    "    wave_periods = []\n",
    "    \n",
    "    for i in 1:(length(zero_crossings)-1)\n",
    "        start_time = zero_crossings[i]\n",
    "        end_time = zero_crossings[i+1]\n",
    "        start_idx = findfirst(t -> t >= start_time, times)\n",
    "        end_idx = findfirst(t -> t >= end_time, times)\n",
    "        wave_segment = elevations[start_idx:end_idx]\n",
    "        peak = maximum(wave_segment)\n",
    "        trough = minimum(wave_segment)\n",
    "        if peak > threshold && trough < -threshold\n",
    "            wave_height = peak - trough\n",
    "            wave_period = end_time - start_time\n",
    "            push!(wave_heights, wave_height)\n",
    "            push!(wave_periods, wave_period)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return(wave_heights, wave_periods)\n",
    "    \n",
    "end    # calculate_valid_wave_heights_periods()\n",
    "\n",
    "# Measure how many standard deviations a value is from the mean\n",
    "function modified_z_score(data, threshold)\n",
    "##########################################\n",
    "    \n",
    "    med = median(data)\n",
    "    mad = median(abs.(data .- med))\n",
    "    mod_z_scores = 0.6745 * (data .- med) ./ mad\n",
    "\n",
    "    outlier_indices = findall(x -> abs(x) > threshold, mod_z_scores)\n",
    "\n",
    "    return(outlier_indices, mod_z_scores)\n",
    "\n",
    "end    # modified_z_score()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "# time stamp each WSE\n",
    "points = collect(0:1:length(heave)-1)/2.56\n",
    "times = []\n",
    "\n",
    "for i in 1:length(points)\n",
    "    push!(times,unix2datetime(datetime2unix(start_date) + points[i]))\n",
    "end\n",
    "\n",
    "elevations = heave\n",
    "\n",
    "threshold = 0\n",
    "\n",
    "exact_zero_up_crossings = find_exact_zero_up_crossings_with_threshold(times, elevations, threshold)\n",
    "wave_heights, wave_periods = calculate_valid_wave_heights_periods(elevations, exact_zero_up_crossings, times, threshold)\n",
    "\n",
    "confidence_interval_95 = 1.96  # For 95% confidence\n",
    "confidence_interval_99 = 2.576  # For 99% confidence\n",
    "\n",
    "start_minute = 0\n",
    "end_minute =  30\n",
    "\n",
    "# Usage for wave heights\n",
    "########################\n",
    "wave_heights_s = wave_heights\n",
    "limits_95_heights = calc_confidence_limits(wave_heights_s, confidence_interval_95)\n",
    "limits_99_heights = calc_confidence_limits(wave_heights_s, confidence_interval_99)\n",
    "\n",
    "# Title and ylabel for wave heights\n",
    "title_wave_heights = \"Wave Heights and Confidence Limits - \" * Dates.format(start_date, \"dd/mm/yyyy HH:MM\")\n",
    "ylabel_wave_heights = \"Wave Height (m)\"\n",
    "\n",
    "# Plot wave heights\n",
    "outlier_indices_heights = plot_outliers(wave_heights_s, exact_zero_up_crossings, limits_95_heights[2], limits_99_heights[2], \n",
    "    title_wave_heights, ylabel_wave_heights, start_minute, end_minute)\n",
    "\n",
    "# Usage for wave periods\n",
    "########################\n",
    "wave_periods_s = [ii.value / 1000 for ii in wave_periods]\n",
    "limits_95_periods = calc_confidence_limits(wave_periods_s, confidence_interval_95)\n",
    "limits_99_periods = calc_confidence_limits(wave_periods_s, confidence_interval_99)\n",
    "\n",
    "# Title and ylabel for wave periods\n",
    "title_wave_periods = \"Wave Periods and Confidence Limits - \" * Dates.format(start_date, \"dd/mm/yyyy HH:MM\")\n",
    "ylabel_wave_periods = \"Wave Period (s)\"\n",
    "\n",
    "# Plot wave periods\n",
    "outlier_indices_periods = plot_outliers(wave_periods_s, exact_zero_up_crossings, limits_95_periods[2], limits_99_periods[2], \n",
    "    title_wave_periods, ylabel_wave_periods, start_minute, end_minute)\n",
    "\n",
    "tm_tick = range(start_date,start_date + Minute(30),step=Minute(1))\n",
    "ticks = Dates.format.(tm_tick,\"MM\")\n",
    "\n",
    "title = \"Zero up-crossing points - \" * Dates.format(start_date, \"dd/mm/yyyy HH:MM\")\n",
    "\n",
    "p1 = plot(title=title, titlefontsize=:10, size=(1200,400),\n",
    "    xlims=(start_date + Minute(start_minute),start_date + Minute(end_minute)), xlabel=\"Record Time (Minutes)\", xlabelfontsize=:10, xticks=(tm_tick,ticks),\n",
    "    ylims=(minimum(heave), maximum(heave)), ylabel=\"WSEs (m)\", ylabelfontsize=:10,  \n",
    "    legend=:topright, leftmargin = 10Plots.mm, bottommargin = 10Plots.mm, fg_legend=:transparent, bg_legend=:transparent, framestyle=:box)\n",
    "\n",
    "p1 = plot!(times, elevations, marker=:circle, ms=:1, malpha=:0.25, alpha=:0.5, label=\"Water Surface Elevation\\n\", lw=:0.5) \n",
    "for jj in outlier_indices_periods\n",
    "    p1 = vline!([exact_zero_up_crossings[jj]], lw=:2, lc=:red, ls=:dash, label=\"\")\n",
    "end\n",
    "p1 = scatter!(exact_zero_up_crossings, zeros(length(exact_zero_up_crossings)), marker=:x, ms=:3, mc=:green, label=\"Exact Zero Up-Crossings\", color=:red)\n",
    "\n",
    "display(p1)\n",
    "\n",
    "gr()\n",
    "using DSP\n",
    "\n",
    "function plot_spectrogram(heave, start_date, condition)\n",
    "################################################\n",
    "\n",
    "    nw=128;\n",
    "    spec = DSP.Periodograms.spectrogram(heave, nw, round(Int, nw*0.9); fs=2.56,window=hanning);\n",
    "    power_spec = DSP.Periodograms.power(spec)\n",
    "    max_spec = maximum(power_spec)\n",
    "    \n",
    "    last_date = start_date + Minute(30)\n",
    "    \n",
    "    # display plots to screen\n",
    "    tm_tick = range(start_date,last_date,step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick,\"MM\")\n",
    "    \n",
    "    #p1 = heatmap(start_date) + Microsecond.(ceil.((spec.time) * 1000000)), spec.freq, power_spec, lw=0.25, c=cgrad(:Spectral, rev=true), clims=(0.0,max_spec), levels=10, fill=true)\n",
    "    p1 = contourf(start_date + Microsecond.(ceil.((spec.time) * 1000000)), spec.freq, power_spec, lw=0.25,\n",
    "        c=cgrad(:Spectral, rev=true), clims=(0,max_spec), levels=10, fill=true)\n",
    "    \n",
    "    # draw grid lines on plot\n",
    "    frequency_grid_lines = 0:0.1:0.6\n",
    "    hline!(p1, frequency_grid_lines, lw=0.5, c=:white, label=\"\")\n",
    "    \n",
    "    # Collect the range of DateTime values into an array\n",
    "    time_grid_lines = collect(start_date:Minute(1):last_date)\n",
    "    vline!(p1, time_grid_lines, lw=0.5, c=:white, label=\"\")\n",
    "    \n",
    "    title = Dates.format.(start_date,\"yyyy-mm-dd HH:MM\") * condition * \" Spectrogram\"\n",
    "    plot_file = replace(\".\\\\Plots\\\\\" * replace(title, \" \" => \"_\") * \".png\", \":\" => \"\")\n",
    "    \n",
    "    plot_p1 = plot(p1, xlabel=\"Record Time (Minutes)\", xlabelfontsize=:10, xlim=(start_date,last_date), xticks=(tm_tick,ticks), xtickfontsize=7,\n",
    "            ylabel=\"Frequency (Hz)\", ylim=(0,0.64), ylabelfontsize=:10, ytickfontsize=8, \n",
    "            title=title, titlefontsize=:10, framestyle = :box,\n",
    "            leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(1200,400), gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, colorbar=:false)\n",
    "#==    \n",
    "    try\n",
    "        # Output plot file name\n",
    "        savefig(plot_file)\n",
    "        println(\"\\nPlot file saved as \",plot_file)\n",
    "    catch\n",
    "        \"Alert: Plot not saved!\"\n",
    "    end\n",
    "==#    \n",
    "    display(plot_p1)\n",
    "\n",
    "    return()\n",
    "\n",
    "end\n",
    "\n",
    "gps_errors_count = 0\n",
    "if gps_errors_count == 0\n",
    "\n",
    "    plot_spectrogram(heave,start_date, \"\")\n",
    "\n",
    "else\n",
    "\n",
    "    plot_spectrogram(heave,start_date, \" Original\")\n",
    "    \n",
    "##    fixed_df = deepcopy(wse_df)\n",
    "##    fixed_df.Heave = fixed_heave\n",
    "    \n",
    "##    plot_spectrogram(fixed_df, \" Fixed\")\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6f670-b4a8-4132-9a9b-7762e6596a60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Statistics\n",
    "\n",
    "function modified_z_score(data, threshold)\n",
    "##########################################\n",
    "    \n",
    "    med = median(data)\n",
    "    mad = median(abs.(data .- med))\n",
    "    mod_z_scores = 0.6745 * (data .- med) ./ mad\n",
    "\n",
    "    outlier_indices = findall(x -> abs(x) > threshold, mod_z_scores)\n",
    "    \n",
    "    return(outlier_indices, mod_z_scores)\n",
    "    \n",
    "end    # modified_z_score()\n",
    "\n",
    "\n",
    "# time stamp each WSE\n",
    "points = collect(0:1:length(heave)-1)/2.56\n",
    "times = []\n",
    "\n",
    "last_date = start_date + Minute(30)\n",
    "    \n",
    "# display plots to screen\n",
    "tm_tick = range(start_date,last_date,step=Minute(1))\n",
    "ticks = Dates.format.(tm_tick,\"MM\")\n",
    "\n",
    "for i in 1:length(points)\n",
    "    push!(times,unix2datetime(datetime2unix(start_date) + points[i]))\n",
    "end\n",
    "    \n",
    "start_minute = 0\n",
    "end_minute = 30\n",
    "\n",
    "# Filter the data within the xlims range\n",
    "time_range_mask = (times .>= start_date + Minute(start_minute)) .& (times .<= start_date + Minute(end_minute))\n",
    "heave_in_range = heave[time_range_mask]\n",
    "\n",
    "# Calculate the ylims based on the filtered heave values\n",
    "ymin = minimum(heave_in_range)\n",
    "ymax = maximum(heave_in_range)\n",
    "padding = 0.05 * (ymax - ymin)\n",
    "ymin_with_padding = ymin - padding\n",
    "ymax_with_padding = ymax + padding\n",
    "\n",
    "last_date = start_date + Minute(30)\n",
    "\n",
    "## Measure how many standard deviations a value is from the mean. \n",
    "## Values with a Z-score above a certain threshold (e.g., ±3) are often considered outliers.\n",
    "threshold = 3\n",
    "outlier_indices, mod_z_scores = modified_z_score(heave, threshold)\n",
    "\n",
    "p1 = plot(xlabel=\"Record Time (Minutes)\", xlabelfontsize=:10, \n",
    "    xlims=(start_date + Minute(start_minute),start_date + Minute(end_minute)), xticks=(tm_tick,ticks), xtickfontsize=7,\n",
    "    ylabel=\"WSEs (m)\", ylim=(ymin_with_padding, ymax_with_padding), ylabelfontsize=:10, ytickfontsize=8, \n",
    "    title=title, titlefontsize=:10, framestyle = :box, fg_legend=:transparent, bg_legend=:transparent, #legend=:topright,\n",
    "    leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(1200,600), gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)\n",
    "\n",
    "p1 = plot!(times, heave, label=\"Heave\", lw=:0.5, lc=:blue, alpha=0.75) \n",
    "\n",
    "##p1 = hline!([lower_bound], ls=:dot, label=\"IRQ bounds\")\n",
    "##p1 = hline!([upper_bound], ls=:dot, label=\"\")\n",
    "\n",
    "confidence_interval_95 = 1.96  # For 95% confidence\n",
    "confidence_interval_99 = 2.576  # For 99% confidence\n",
    "\n",
    "limits_95 = calc_confidence_limits(heave, confidence_interval_95)\n",
    "limits_99 = calc_confidence_limits(heave, confidence_interval_99)\n",
    "\n",
    "outlier_indices, mod_z_scores = modified_z_score(heave, 3)\n",
    "# Check if there are outliers and plot them if any are positive\n",
    "if !isempty(outlier_indices) && any(x -> x > 0, mod_z_scores[outlier_indices])\n",
    "    scatter!(p1, times[outlier_indices], heave[outlier_indices], label=\"Possible Outliers\", marker=:circle, color=:red)\n",
    "end\n",
    "\n",
    "p1 = hline!(p1, [limits_99[1]], color=:red, lw=:1, linestyle=:dash, label=\"99% Confidence Upper Limit\")\n",
    "p1 = hline!(p1, [limits_95[1]], color=:green, lw=:1, linestyle=:dash, label=\"95% Confidence Upper Limit\")\n",
    "\n",
    "p1 = hline!(p1, [limits_99[2]], color=:red, lw=:1, linestyle=:dash, label=\"\")\n",
    "p1 = hline!(p1, [limits_95[2]], color=:green, lw=:1, linestyle=:dash, label=\"\")\n",
    "\n",
    "display(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab956269-a2f3-4afa-a0e0-398489063c2d",
   "metadata": {},
   "source": [
    "### Initial Code for Training an Autoencoder in Julia (using Flux.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79778a-518b-49b3-9f7c-2980ec78bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "\n",
    "function min_max_normalize_matrix(X)\n",
    "    min_vals = minimum(X, dims=1)  # Compute min for each column\n",
    "    max_vals = maximum(X, dims=1)  # Compute max for each column\n",
    "    return (X .- min_vals) ./ (max_vals .- min_vals)\n",
    "end\n",
    "\n",
    "\n",
    "function z_score_normalize_matrix(X)\n",
    "    mean_vals = mean(X, dims=1)  # Mean for each column\n",
    "    std_vals = std(X, dims=1)    # Standard deviation for each column\n",
    "    return (X .- mean_vals) ./ std_vals\n",
    "end\n",
    "\n",
    "\n",
    "function pad_or_truncate(record, target_length=4608)\n",
    "####################################################\n",
    "#==    \n",
    "    if length(record) < target_length\n",
    "        # Pad with zeros (or any other value you prefer)\n",
    "        return vcat(record, zeros(Float32, target_length - length(record)))\n",
    "    elseif length(record) > target_length\n",
    "        # Truncate to the target length\n",
    "        return record[1:target_length]\n",
    "    else\n",
    "        return record\n",
    "    end\n",
    "==#\n",
    "    length(record) < target_length ? vcat(record, zeros(Float32, target_length - length(record))) :\n",
    "                                     record[1:target_length]\n",
    "\n",
    "end    # pad_or_truncate()\n",
    "\n",
    "\n",
    "function get_heave(Data, f23_df)\n",
    "################################\n",
    "    \n",
    "    heave_array = []\n",
    "    X_date = []\n",
    "    \n",
    "    for idx in 1:nrow(f23_df)\n",
    "\n",
    "        if !isnothing(f23_df.Data_vector[idx])\n",
    "    \n",
    "            start_date, start_val, end_val = get_start_end_dates(f23_df,idx)\n",
    "            if start_val > 0\n",
    "                print(\".\")\n",
    "                heave, north, west = get_hnw(Data,start_val,end_val)\n",
    "\n",
    "                # ensure we have 4608 data points\n",
    "                push!(heave_array,pad_or_truncate(heave, 4608))\n",
    "                push!(X_date,start_date)\n",
    "            end\n",
    "\n",
    "        end\n",
    "    \n",
    "    end\n",
    "\n",
    "    return(hcat(heave_array...), X_date)\n",
    "\n",
    "end    # get_heave()\n",
    "\n",
    "\n",
    "function calc_reconstruction_errors(X_train_float32, model)\n",
    "####################################################\n",
    "    \n",
    "    reconstruction_errors = Float32[]\n",
    "    \n",
    "    for record in eachcol(X_train_float32)  # Each record is now a column with 14 features\n",
    "        reconstructed_record = model(record)  # Pass the record to the autoencoder\n",
    "        error = mean((reconstructed_record .- record).^2)  # Calculate the reconstruction error\n",
    "        push!(reconstruction_errors, error)  # Store the error\n",
    "    end\n",
    "    \n",
    "    return(reconstruction_errors)\n",
    "\n",
    "end    # calc_reconstruction_errors()\n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "@time begin\n",
    "# Define autoencoder model\n",
    "model = Chain(\n",
    "    Dense(4608, 128, relu),  # Encoder\n",
    "    Dense(128, 64, relu),    # Bottleneck\n",
    "    Dense(64, 128, relu),    # Decoder\n",
    "    Dense(128, 4608)         # Output layer, reconstructs input\n",
    ")\n",
    "\n",
    "# Define the loss function (e.g., Mean Squared Error for reconstruction)\n",
    "loss(x) = Flux.mse(model(x), x)\n",
    "\n",
    "# Optimizer: Adam with default parameters (learning rate, etc.)\n",
    "opt = Adam()\n",
    "   \n",
    "X_train, X_date = get_heave(Data, f23_df)\n",
    "\n",
    "# Normalize your training data\n",
    "##X_train_normalized = normalize_records(X_train)\n",
    "X_train_normalized = min_max_normalize_matrix(X_train)\n",
    "    \n",
    "# Convert WSE data to Float32\n",
    "X_train_float32 = Float32.(X_train_normalized)\n",
    "\n",
    "# calculate the reconstruction_errors\n",
    "reconstruction_errors_model = calc_reconstruction_errors(X_train_float32, model)\n",
    "\n",
    "# Use the converted data for training\n",
    "data = Iterators.repeated((X_train_float32,), 100)  # Example of data iteration for training\n",
    "\n",
    "# Train the model\n",
    "Flux.train!(loss, Flux.params(model), data, opt)\n",
    "end    # @time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15119f79-ac7e-4be0-ac37-6ce55508c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(reconstruction_errors_model)\n",
    "# display plots to screen\n",
    "tm_tick = range(X_date[1],X_date[end],step=Hour(1))\n",
    "ticks = Dates.format.(tm_tick,\"HH\")\n",
    "\n",
    "plot(X_date, reconstruction_errors_model, size = (1200,600), xticks=(tm_tick,ticks), label=\"Error model\", framestyle=:box)\n",
    "hline!([mean(reconstruction_errors_model)], label=\"Mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f6db6-d8ed-458f-858c-737d5916b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, test on new WSE data and check for large reconstruction errors\n",
    "new_wse_record = heave\n",
    "new_wse_record_normalized = min_max_normalize(new_wse_record)\n",
    "# Convert WSE data to Float32\n",
    "new_wse_record_normalized_float32 = Float32.(new_wse_record_normalized)\n",
    "reconstruction_error = mean((model(new_wse_record_normalized_float32) .- new_wse_record_normalized_float32).^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe712f-24d6-4f30-9d8b-d49265b6b832",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mean_error = mean(reconstruction_errors)  # where reconstruction_errors is an array of errors from the training data\n",
    "std_error = std(reconstruction_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9684fd-eb36-4e82-a92f-c112ef3de2c3",
   "metadata": {},
   "source": [
    "### Save model, optimiser, and normalised heave data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1472bf3-52f6-4702-b22a-6ef86eb515d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, JLD2\n",
    "\n",
    "# Save the model and optimizer\n",
    "model_state = Flux.state(model)\n",
    "opt_state = opt # Flux.setup(Adam(), model)\n",
    "\n",
    "outfil = \"HVA_model_\"*Dates.format(now(), \"yyyy_mm_dd_HHMM\")*\".jld2\" \n",
    "\n",
    "# Save model and optimizer and normalised wave data to a JLD2 file\n",
    "jldsave(outfil; model_state, opt_state, X_train_float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f8979-a533-41ce-84bc-bb2b469d3960",
   "metadata": {},
   "source": [
    "### Recover saved model, optimiser, and normalised heave data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2, Flux\n",
    "\n",
    "# Load the model and optimizer states from the JLD2 file\n",
    "infil = outfil  # Replace with your actual file name\n",
    "loaded_data = jldopen(infil, \"r\") do file\n",
    "    old_model_state = file[\"model_state\"]  # Load model state\n",
    "    old_opt_state = file[\"opt_state\"]      # Load optimizer state\n",
    "    old_X_train_float32 = file[\"X_train_float32\"]  # Load the previous X_train_float32 data\n",
    "    return (old_model_state, old_opt_state, old_X_train_float32) # Return all states and data\n",
    "end\n",
    "\n",
    "old_model_state, old_opt_state, old_X_train_float32 = loaded_data\n",
    "\n",
    "# Define the old model architecture\n",
    "old_model = Chain(\n",
    "    Dense(4608, 128, relu),  # Encoder\n",
    "    Dense(128, 64, relu),    # Bottleneck\n",
    "    Dense(64, 128, relu),     # Decoder\n",
    "    Dense(128, 4608)          # Output layer, reconstructs input\n",
    ")\n",
    "\n",
    "# Restore model parameters from the loaded state\n",
    "for (layer, state) in zip(old_model.layers, old_model_state[:layers])\n",
    "    layer.weight .= state.weight   # Assign saved weights\n",
    "    layer.bias .= state.bias       # Assign saved biases\n",
    "end\n",
    "\n",
    "# Restore the optimizer state directly\n",
    "old_opt = old_opt_state;  # Assign the loaded optimizer state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049e785",
   "metadata": {},
   "source": [
    "### Check 30-minute records are suitable for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in 1:size(X_train_float32)[2]\n",
    "    \n",
    "    p1 = plot(X_train_float32[:,ii], size=(2400,200), label=Dates.format(X_date[ii], \"yyyy_mm_dd_HHMM\"),\n",
    "        framestyle = :box, fg_legend=:transparent, bg_legend=:transparent, legend=:topright)\n",
    "        \n",
    "    display(p1)\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51195d62-1efa-43ce-a8fd-fc5023564edb",
   "metadata": {},
   "source": [
    "### Append new data to existing model and retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8e81e-e9fe-4c83-84a3-7dccd769315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using JLD2\n",
    "\n",
    "# Step 1: Load the existing model and optimizer\n",
    "loaded_model, opt = JLD2.@load \"autoencoder_model.jld2\" model opt\n",
    "\n",
    "# Step 2: Load new records and prepare the data\n",
    "new_X_train, _ = get_heave(Data, f23_df)  # Replace with your new data fetching function\n",
    "new_X_train_normalized = normalize_records(new_X_train)\n",
    "new_X_train_float32 = Float32.(new_X_train_normalized)\n",
    "\n",
    "# Step 3: Combine with the previous training data (if applicable)\n",
    "# You can concatenate with previous training data if desired\n",
    "X_combined = hcat(old_X_train_float32, new_X_train_float32)\n",
    "\n",
    "# Step 4: Define the loss function again\n",
    "loss(x) = Flux.mse(loaded_model(x), x)\n",
    "\n",
    "# Prepare the data for training (iterating over the new combined data)\n",
    "data = Iterators.repeated((X_combined,), 100)\n",
    "\n",
    "# Step 5: Train the model on the new data\n",
    "Flux.train!(loss, Flux.params(loaded_model), data, opt)\n",
    "\n",
    "# Optionally save the updated model and optimizer state\n",
    "#JLD2.@save \"updated_autoencoder_model.jld2\" loaded_model opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e8cb1-3ba3-4bf9-9fb9-2d6ea58c8067",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Flux\n",
    "\n",
    "# Function to preprocess new data (normalize, pad, etc.)\n",
    "function preprocess_new_data(new_raw_data)\n",
    "    # Apply the same normalization and padding used for the original data\n",
    "    new_data_normalized = normalize_records(new_raw_data)  # Adjust this to your preprocessing function\n",
    "    new_data_float32 = Float32.(new_data_normalized)        # Convert to Float32\n",
    "    return new_data_float32\n",
    "end\n",
    "\n",
    "# Example new data (replace with your actual new data)\n",
    "new_raw_data = rand(Float32, 4608, 10)  # Example new data (10 new records)\n",
    "X_new = preprocess_new_data(new_raw_data)\n",
    "\n",
    "# Combine old and new data if desired\n",
    "X_combined = hcat(X_train_float32, X_new)  # Combine with old training data\n",
    "\n",
    "# Define the loss function\n",
    "loss(x) = Flux.mse(new_model(x), x)  # Use the existing model\n",
    "\n",
    "# Create a data iterator for training\n",
    "data = Iterators.repeated((X_combined,), 100)  # Use the combined data for training\n",
    "\n",
    "# Continue training the model with the new data\n",
    "Flux.train!(loss, Flux.params(new_model), data, new_opt)  # new_opt is the optimizer state loaded earlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc118543-2477-4d95-b503-1733c53f3ff3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40acb854",
   "metadata": {},
   "source": [
    "## Generate Datawell Double Integration and band Pass Filter for DWR4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be839247",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Printf\n",
    "using DSP,Plots\n",
    "using SpecialFunctions\n",
    "using QuadGK\n",
    "\n",
    "fs = 5.12\n",
    "fₗₒ = 0.03333\n",
    "fᵤₚ = 1\n",
    "M = 256\n",
    "alpha = 9.3\n",
    "P0 = 123\n",
    "color1 = \"darkred\"\n",
    "color2 = \"yellow\"\n",
    "label = \"DWR4\"\n",
    "\n",
    "# calc f0\n",
    "f0 = 1 / P0\n",
    "\n",
    "# create empty vector\n",
    "dt = zeros(0)\n",
    "dt1 = zeros(0)\n",
    "\n",
    "# create vector of time values over length of filter\n",
    "m = M + 1  #Int(round(M/2))\n",
    "time = [-m:2:m;]\n",
    "\n",
    "print(\"Processing \",label,\" now.\\n\")\n",
    "# integrate filter expression over range of values - formula (7) in Datawell notes\n",
    "for t in time\n",
    "    DWR_df(x) = cos(2*pi*x*t) / (x^2 + f0^2);\n",
    "    append!(dt,-1/(2*pi^2)*quadgk(DWR_df,fₗₒ,fᵤₚ)[1])\n",
    "\n",
    "    # calculate check result using sine and cosine integrals\n",
    "    x0 = 2* pi * f0 * t\n",
    "    x1 = 2* pi * fᵤₚ * t \n",
    "    x2 = 2* pi * fₗₒ * t  \n",
    "\n",
    "    append!(dt1,t/pi*(sinh(x0)/2/x0 * (sinint(x1 + real(1im*x0))  + sinint(x1 - real(1im*x0))) -\n",
    "                   real(1im*cosh(x0))/2/x0 * (cosint(abs(x1 + real(1im*x0))) - cosint(abs(x1 - 1im*x0)))) - \n",
    "               t/pi*(sinh(x0)/2/x0 * (sinint(x2 + real(1im*x0))  + sinint(x2 - real(1im*x0))) -\n",
    "                   real(1im*cosh(x0))/2/x0 * (cosint(abs(x2 + 1im*x0)) - cosint(abs(x2 - 1im*x0)))))\n",
    "\n",
    "end \n",
    "\n",
    "# generate a Kaiser window over filter range using alpha value\n",
    "window_function = kaiser(M+2, alpha)\n",
    "\n",
    "filter = dt.*window_function/fs .* -1\n",
    "\n",
    "# plot filter with Kaiser window applied to remove Gibbs phenomenon\n",
    "global dwr_g_filter = dt.*window_function/fs\n",
    "p1 = plot(time,filter,title=\"Datawell filters\", \n",
    "    c = color1, lw = 3, xlim=(-256,256), xticks=(-250:25:250), yticks=(-0.6:0.1:0.2),\n",
    "    label = label, fg_legend = :false, legendfont=font(16),\n",
    "    size = (950, 650), framestyle = :box)\n",
    "#==\n",
    "else\n",
    "    mk4_filter = dt.*window_function/fs\n",
    "    p2 = display(plot!(time,dt.*window_function/fs, \n",
    "        c = color1, lw = 2, \n",
    "        label = label, fg_legend = :false, legendfont=font(16),\n",
    "        size = (950, 650), framestyle = :box))\n",
    "end\n",
    "==#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bd75f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using StatsBase\n",
    "\n",
    "plot(autocor(filter; demean=true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b4f8f",
   "metadata": {},
   "source": [
    "## Investigate segmenting WSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb616b-f220-4915-8fda-3825edba24e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "function get_start_end_dates(f23_df,found_list)   \n",
    "    start_date = f23_df[found_list[1],:].Date - Minute(30) # <------- NOTE subtracted 30min from start_date to match Waves4 results\n",
    "    segments = f23_df[found_list[1],:].Segments\n",
    "#   match_vector = f23_df[found_list[1],:].Match_vector\n",
    "    sample_nos = f23_df[found_list[1],:].Sample_number\n",
    "    data_vector = f23_df[found_list[1],:].Data_vector\n",
    "    start_val = data_vector - Int(sample_nos/2) + 1\n",
    "    end_val = data_vector\n",
    "    \n",
    "    return(start_date,start_val, end_val)\n",
    "    \n",
    "end    #(get_start_end_dates)\n",
    "\n",
    "\n",
    "function spike_value(wse)\n",
    "#####################################    \n",
    "    median_value = median(wse)\n",
    "    std_value = std(wse)\n",
    "\n",
    "    return(median_value + 3*std_value)\n",
    "\n",
    "    end    # spike_value()\n",
    "\n",
    "# 18 for 0730\n",
    "# 26 for 1230 - segments 10, 11, 12\n",
    "\n",
    "start_date, start_val, end_val = get_start_end_dates(f23_df, 16)\n",
    "heave, north, west = get_hnw(Data,start_val,end_val);\n",
    "\n",
    "spike = spike_value(heave)\n",
    "heave_spikes = findall(i->(i>=spike), abs.(heave));\n",
    "\n",
    "spike = spike_value(north)\n",
    "north_spikes = findall(i->(i>=spike), abs.(north));\n",
    "\n",
    "spike = spike_value(west)\n",
    "west_spikes = findall(i->(i>=spike), abs.(west));\n",
    "\n",
    "# time stamp each WSE\n",
    "points = collect(0:1:length(heave)-1)/2.56\n",
    "times = []\n",
    "\n",
    "for i in 1:length(points)\n",
    "    push!(times,unix2datetime(datetime2unix(start_date) + points[i]))\n",
    "end\n",
    "\n",
    "# create plots of heave, north, and west\n",
    "title_string = Dates.format(start_date, \"dd/mm/yyyy HH:MM\")\n",
    "p1_hnw = scatter(times[heave_spikes], heave[heave_spikes], label=\"\", markershape=:circle, ms=4, mc=:white, ma=1, msc=:red, msa=0.25, msw=0.5)\n",
    "p1_hnw = plot!(times,heave, label=\"\", c=\"#4a536b\", lw=0.5, title=title_string, titlefontsize=12) ##last(split(infil,\"\\\\\")))\n",
    "\n",
    "# get plotting limits\n",
    "x_lim1 = xlims(p1_hnw)[1]; y_lim1 = ylims(p1_hnw)[1]\n",
    "x_lim2 = xlims(p1_hnw)[2]; y_lim2 = ylims(p1_hnw)[2]\n",
    "\n",
    "x_pos = x_lim1 + abs(x_lim2-x_lim1)*0.02\n",
    "p1_hnw = annotate!(x_pos, y_lim2*1.1, text(\"Firmwave ver. = \" * fc1_df.Firmware[1], :grey, :left, 7))\n",
    "x_pos = x_lim1 + abs(x_lim2-x_lim1)*0.13\n",
    "p1_hnw = annotate!(x_pos, y_lim2*1.1, text(\"Hatch UID = \" * string(fc1_df.Hatch_uid[1]), :grey, :left, 7))\n",
    "x_pos = x_lim1 + abs(x_lim2-x_lim1)*0.26\n",
    "p1_hnw = annotate!(x_pos, y_lim2*1.1, text(\"Hull UID = \" * string(fc1_df.Hull_uid[1]), :grey, :left, 7))\n",
    "\n",
    "p2_hnw = scatter(times[north_spikes], north[north_spikes], label=\"\", markershape=:circle, ms=4, mc=:white, ma=1, msc=:red, msa=0.25, msw=0.5)\n",
    "p2_hnw = plot!(times,north, label=\"\", c=\"#aed6dc\", lw=0.5)\n",
    "p3_hnw = scatter(times[west_spikes], west[west_spikes], label=\"\", markershape=:circle, ms=4, mc=:white, ma=1, msc=:red, msa=0.25, msw=0.5)\n",
    "p3_hnw = plot!(times,west, label=\"\", c=\"#ff9a8d\", lw=0.5)\n",
    "\n",
    "hline!(p1_hnw, [0], lw=1, label=\"\")\n",
    "hline!(p2_hnw, [0], lw=1, label=\"\")\n",
    "hline!(p3_hnw, [0], lw=1, label=\"\")\n",
    "\n",
    "# get plotting limits\n",
    "x_lim1 = xlims(p1_hnw)[1]; y_lim1 = ylims(p1_hnw)[1]\n",
    "x_lim2 = xlims(p1_hnw)[2]; y_lim2 = ylims(p1_hnw)[2]\n",
    "\n",
    "# display plots to screen\n",
    "plot_wse = Plots.plot(p1_hnw, p2_hnw, p3_hnw, layout = (3, 1), size = (1800, 1000),\n",
    "    xlim=(first(times),last(times)),  xticks = first(times):Minute(5):last(times),xtickfontsize=7,ytickfontsize=8,\n",
    "    framestyle = :box, fg_legend=:transparent, legend=:bottomleft,\n",
    "    margin = 1Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)            \n",
    "\n",
    "display(plot_wse)\n",
    "\n",
    "#plot_hnw(f23_df,fc1_df,Data,26)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a981351-fbaf-4b6c-88cb-b176381b98c2",
   "metadata": {},
   "source": [
    "using DSP\n",
    "\n",
    "heave_segments = arraysplit(heave, 512, 256);\n",
    "aa = times\n",
    "n = 512\n",
    "time_segments = [aa[i:min(i + n - 1, end)] for i in 1:n:length(aa)];\n",
    "\n",
    "p1 = plot(time_segments[1], heave_segments[1], label=\"\", framestyle = :box)\n",
    "p2 = plot(time_segments[3], heave_segments[3], label=\"\", framestyle = :box)\n",
    "#p1 = plot!(time.+m.+20,filter * 15, label=\"\")\n",
    "\n",
    "plot_p1 = plot(p1, p2, layout=(1,2), size = (1800,200), bottommargin = 20Plots.mm)\n",
    "\n",
    "display(plot_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3499e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "aa = times\n",
    "n = 512\n",
    "bb = [aa[i:min(i + n - 1, end)] for i in 1:n:length(aa)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd426e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "aa = times\n",
    "n = 17\n",
    "cc = [aa[i:min(i + n - 1, end)] for i in 1:n:length(aa)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a88dc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ace7ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "function get_start_end_dates(f23_df,found_list)   \n",
    "    start_date = f23_df[found_list[1],:].Date - Minute(30) # <------- NOTE subtracted 30min from start_date to match Waves4 results\n",
    "    segments = f23_df[found_list[1],:].Segments\n",
    "#   match_vector = f23_df[found_list[1],:].Match_vector\n",
    "    sample_nos = f23_df[found_list[1],:].Sample_number\n",
    "    data_vector = f23_df[found_list[1],:].Data_vector\n",
    "    start_val = data_vector - Int(sample_nos/2) + 1\n",
    "    end_val = data_vector\n",
    "    \n",
    "    return(start_date,start_val, end_val)\n",
    "    \n",
    "end    #(get_start_end_dates)\n",
    "\n",
    "\n",
    "function spike_value(wse)\n",
    "#####################################    \n",
    "    median_value = median(wse)\n",
    "    std_value = std(wse)\n",
    "\n",
    "    return(median_value + 3*std_value)\n",
    "\n",
    "    end    # spike_value()\n",
    "\n",
    "\n",
    "start_date, start_val, end_val = get_start_end_dates(f23_df, 26)\n",
    "heave, north, west = get_hnw(Data,start_val,end_val);\n",
    "title_string = Dates.format(start_date, \"dd/mm/yyyy HH:MM\")\n",
    "\n",
    "spike = spike_value(heave)\n",
    "heave_spikes = findall(i->(i>=spike), abs.(heave));\n",
    "\n",
    "spike = spike_value(north)\n",
    "north_spikes = findall(i->(i>=spike), abs.(north));\n",
    "\n",
    "spike = spike_value(west)\n",
    "west_spikes = findall(i->(i>=spike), abs.(west));\n",
    "\n",
    "# time stamp each WSE\n",
    "points = collect(0:1:length(heave)-1)/2.56\n",
    "times = []\n",
    "\n",
    "for i in 1:length(points)\n",
    "    push!(times,unix2datetime(datetime2unix(start_date) + points[i]))\n",
    "end\n",
    "\n",
    "xpos = times[1]\n",
    "ypos = maximum(heave)\n",
    "    \n",
    "# display plots to screen\n",
    "tm_tick = range(first(times),last(times),step=Minute(5))\n",
    "ticks = Dates.format.(tm_tick,\"HH:MM:SS\")    \n",
    "\n",
    "pp = plot(times, heave, size = (1800,400), xlim=(first(times),last(times)), xticks=(tm_tick,ticks), title=title_string, titlefontsize=12, bottommargin = 10Plots.mm, leftmargin = 0Plots.mm, label=\"\", framestyle = :box)\n",
    "pp = annotate!(xpos, ypos, text(\"  \"*last(split(infil, \"\\\\\"; limit=10)), :red, :left, 8))\n",
    "\n",
    "display(pp)\n",
    "\n",
    "odds = collect(Iterators.partition(times, 512));\n",
    "evens = collect(Iterators.partition(times[n+1-256:end-n], n));\n",
    "#reshape([odds evens]', length(odds)+length(evens))\n",
    "\n",
    "date_segments = []\n",
    "\n",
    "ii = 1\n",
    "while ii <= 8\n",
    "    push!(date_segments,odds[ii])\n",
    "    push!(date_segments,evens[ii])\n",
    "    ii+=1\n",
    "end\n",
    "\n",
    "push!(date_segments,odds[ii]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18cc40e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "segment = 11\n",
    "\n",
    "p1 = plot(date_segments[segment-1], heave_segments[segment-1], label=\"\", framestyle = :box)\n",
    "p2 = plot(date_segments[segment], heave_segments[segment], label=\"\", framestyle = :box)\n",
    "p3 = plot(date_segments[segment+1], heave_segments[segment+1], label=\"\", framestyle = :box)\n",
    "\n",
    "spectra1 = periodogram(heave_segments[segment-1]; onesided=eltype(heave_segments[segment-1])<:Real, nfft=nextfastfft(n), fs=2.56, window=nothing);\n",
    "spectra2 = periodogram(heave_segments[segment]; onesided=eltype(heave_segments[segment])<:Real, nfft=nextfastfft(n), fs=2.56, window=nothing);\n",
    "spectra3 = periodogram(heave_segments[segment+1]; onesided=eltype(heave_segments[segment+1])<:Real, nfft=nextfastfft(n), fs=2.56, window=nothing);\n",
    "\n",
    "s1 = plot(freq(spectra1), power(spectra1), label=\"\", title=\"Segment \"*string(segment-1), titlefontsize=8, framestyle = :box)\n",
    "s2 = plot(freq(spectra2), power(spectra2), label=\"\", title=\"Segment \"*string(segment), titlefontsize=8, framestyle = :box)\n",
    "s3 = plot(freq(spectra3), power(spectra3), label=\"\", title=\"Segment \"*string(segment+1), titlefontsize=8, framestyle = :box)\n",
    "\n",
    "plot_p1 = plot(p1, p2, p3, s1, s2, s3, layout=(2,3), size = (1800,800), bottommargin = 10Plots.mm, leftmargin = 0Plots.mm, ylabel=\"\")\n",
    "\n",
    "display(plot_p1)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14fc1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# refer to https://github.com/lnacquaroli/SavitzkyGolay.jl\n",
    "using SavitzkyGolay\n",
    "\n",
    "segment = 8\n",
    "\n",
    "sg = savitzky_golay(heave_segments[segment], 51, 3)\n",
    "\n",
    "result = heave_segments[segment] - sg.y\n",
    "\n",
    "p2 = plot(date_segments[segment], heave_segments[segment], lw=:4, color=:yellow, label=\"Heave signal\")\n",
    "p2 = plot!(date_segments[segment], sg.y, ls=:dot, label=\"Filtered\")\n",
    "\n",
    "p2 = plot!(date_segments[segment], result, lw=:2, color=:red, label=\"Filtered result\")\n",
    "\n",
    "\n",
    "\n",
    "spectra2 = periodogram(result; onesided=eltype(result)<:Real, nfft=nextfastfft(n), fs=2.56, window=nothing);\n",
    "s2 = plot(freq(spectra2), power(spectra2), label=\"\", framestyle = :box)\n",
    "\n",
    "p2_plot = plot(p2, s2, size=(800,600), layout=(2,1) , fg_legend=:transparent, bg_legend=:transparent, legend=:topright,\n",
    "        leftmargin = 10Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, show=true, framestyle = :box)\n",
    "\n",
    "display(p2_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9a261",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# refer to https://github.com/lnacquaroli/SavitzkyGolay.jl\n",
    "using SavitzkyGolay\n",
    "\n",
    "segment = 8\n",
    "sg1 = savitzky_golay(heave_segments[segment-1], 51, 3)\n",
    "sg2 = savitzky_golay(heave_segments[segment], 51, 3)\n",
    "sg3 = savitzky_golay(heave_segments[segment+1], 51, 3)\n",
    "\n",
    "result1 = heave_segments[segment-1] - sg1.y\n",
    "result2 = heave_segments[segment] - sg2.y\n",
    "result3 = heave_segments[segment+1] - sg3.y\n",
    "\n",
    "p1 = plot(date_segments[segment-1], result1, label=\"\", title=\"Segment \"*string(segment-1), titlefontsize=8, framestyle = :box)\n",
    "p2 = plot(date_segments[segment], result2, label=\"\", title=\"Segment \"*string(segment), titlefontsize=8, framestyle = :box)\n",
    "p3 = plot(date_segments[segment+1], result3, label=\"\", title=\"Segment \"*string(segment+1), titlefontsize=8, framestyle = :box)\n",
    "\n",
    "spectra1 = periodogram(result1; onesided=eltype(heave_segments[segment-1])<:Real, nfft=nextfastfft(n), fs=2.56, window=nothing);\n",
    "spectra2 = periodogram(result2; onesided=eltype(heave_segments[segment])<:Real, nfft=nextfastfft(n), fs=2.56, window=nothing);\n",
    "spectra3 = periodogram(result3; onesided=eltype(heave_segments[segment+1])<:Real, nfft=nextfastfft(n), fs=2.56, window=nothing);\n",
    "\n",
    "s1 = plot(freq(spectra1), power(spectra1), label=\"\", title=\"Segment \"*string(segment-1), titlefontsize=8, framestyle = :box)\n",
    "s2 = plot(freq(spectra2), power(spectra2), label=\"\", title=\"Segment \"*string(segment), titlefontsize=8, framestyle = :box)\n",
    "s3 = plot(freq(spectra3), power(spectra3), label=\"\", title=\"Segment \"*string(segment+1), titlefontsize=8, framestyle = :box)\n",
    "\n",
    "plot_p1 = plot(p1, p2, p3, s1, s2, s3, layout=(2,3), size = (1800,800), bottommargin = 10Plots.mm, leftmargin = 0Plots.mm, ylabel=\"\")\n",
    "\n",
    "display(plot_p1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a68d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# refer to https://github.com/lnacquaroli/SavitzkyGolay.jl\n",
    "using SavitzkyGolay\n",
    "\n",
    "gr() \n",
    "\n",
    "displacement = heave\n",
    "\n",
    "# determine length of the filter window\n",
    "#==\n",
    "if maximum(displacement) > 2\n",
    "    window_size = 31\n",
    "elseif maximum(displacement) > 6\n",
    "    window_size = 51\n",
    "else\n",
    "    window_size = 41\n",
    "end\n",
    "==#\n",
    "window_size = 21 #<============ Try this until least squares fit developed\n",
    "\n",
    "xpos = times[1]\n",
    "ypos = maximum(displacement)\n",
    "\n",
    "m = length(displacement)\n",
    "sg = savitzky_golay(displacement, window_size, 3);\n",
    "\n",
    "# find where peak in savitzky_golay curve\n",
    "location = findmax(sg.y)[2]\n",
    "\n",
    "# set lower and upper limits of savitzky_golay filter\n",
    "if location-255 > 1\n",
    "    lower = location-255\n",
    "else\n",
    "    lower = 1\n",
    "end\n",
    "\n",
    "if location+255 < length(displacement)\n",
    "    upper = location+255\n",
    "else\n",
    "    upper = length(displacement)\n",
    "end\n",
    "\n",
    "# set savitzky_golay filter to zero outside limits\n",
    "sg.y[1:lower] .= 0\n",
    "sg.y[upper:end] .= 0\n",
    "result = displacement-sg.y\n",
    "\n",
    "outfil = rsplit(infil, \"\\\\\"; limit=2)[2][1:end-4]        \n",
    "\n",
    "# display plots to screen\n",
    "tm_tick = range(first(times),last(times),step=Minute(5))\n",
    "ticks = Dates.format.(tm_tick,\"HH:MM\")\n",
    "\n",
    "pp1 = plot(times, displacement, label=\"Uncorrected WSE\", title=title_string, titlefontsize=12)\n",
    "pp1 = plot!(times, sg.y, label=\"Filter\", lc=:red, ls=:dot)\n",
    "pp1 = vline!([times[location]], lw=1, lc=:red, ls=:dash, label=\"\")\n",
    "#pp1 = plot!(times, sg.y, label=\"\")\n",
    "pp1 = annotate!(xpos, ypos, text(\"  \"*outfil, :red, :left, 8))\n",
    "\n",
    "pp2 = plot(times, displacement, lw=:2, lc=:yellow, label=\"Uncorrected WSE\")        \n",
    "pp2 = plot!(times, result, lw=0.5, lc=:blue, label=\"Corrected WSE\")\n",
    "\n",
    "plot_p1 = plot(pp1, pp2, layout=(2,1), size = (1800,800), xlim=(first(times),last(times)), xticks=(tm_tick,ticks),\n",
    "    fg_legend=:transparent, legend=:bottomleft, bottommargin = 10Plots.mm, leftmargin = 0Plots.mm, ylabel=\"\", framestyle = :box)\n",
    "\n",
    "savefig(\".\\\\\"*outfil*\"_filtered_WSE\"*Dates.format(start_date, \"_HHMM\")*\".png\")\n",
    "\n",
    "display(plot_p1) \n",
    "\n",
    "spectra1 = periodogram(displacement; onesided=eltype(displacement)<:Real, nfft=nextfastfft(m), fs=2.56, window=nothing);\n",
    "spectra3 = periodogram(result; onesided=eltype(displacement)<:Real, nfft=nextfastfft(m), fs=2.56, window=nothing);\n",
    "\n",
    "spectra1w = welch_pgram(displacement, 512, 256; onesided=true, nfft=512, fs=2.56, window=hanning)\n",
    "spectra1_sg = welch_pgram(sg.y, 512, 256; onesided=true, nfft=512, fs=2.56, window=hanning)\n",
    "spectra3w = welch_pgram(result, 512, 256; onesided=true, nfft=512, fs=2.56, window=hanning)\n",
    "\n",
    "Tp1 = 1/freq(spectra1w)[findmax(power(spectra1w))[2]]\n",
    "Tp3 = 1/freq(spectra3w)[findmax(power(spectra3w))[2]]    \n",
    "    \n",
    "#s1 = plot(freq(spectra1), power(spectra1), label=\"\", framestyle = :box)\n",
    "s1 = plot(freq(spectra1w), power(spectra1w), xlim=(0,0.64), ylim=(floor(minimum(power(spectra1w))),ceil(maximum(power(spectra1w)))), \n",
    "        title=outfil*\" - \"*title_string*\" Uncorrected\", titlefontsize=10,\n",
    "        lw=:2, lc=:blue, label=\"Uncorrected spectra\\n\",fillrange = 0, fillalpha = 0.075, fillcolor = :blue, framestyle = :box)\n",
    "s1 = plot!(freq(spectra1_sg), power(spectra1_sg), lw=:1, lc=:red, ls=:dash, label=\"\")\n",
    "s1 = vline!([freq(spectra1w)[findmax(power(spectra1w))[2]]], lw=1, lc=:grey, ls=:dash, label=\"Tp = \"*string(round.(Tp1; digits=2))*\"s\")\n",
    "\n",
    "s3 = plot(freq(spectra1_sg), power(spectra1_sg), xlim=(0,0.64), ylim=(floor(minimum(power(spectra1w))),ceil(maximum(power(spectra1w)))),\n",
    "        title=outfil*\" - \"*title_string*\" Corrected\", titlefontsize=10,        \n",
    "        lw=:1, lc=:red, ls=:dash, label=\"\",fillrange = 0, fillalpha = 0.025, fillcolor = :red)\n",
    "s3 = plot!(freq(spectra3w), power(spectra3w), lw=:2, lc=:blue, label=\"Corrected spectra\\n\",fillrange = 0, fillalpha = 0.075, fillcolor = :blue)\n",
    "s3 = vline!([freq(spectra3w)[findmax(power(spectra3w))[2]]], lw=1, lc=:grey, ls=:dash, label=\"Tp = \"*string(round.(Tp3; digits=2))*\"s\")\n",
    "\n",
    "plot_s1 = plot(s1, s3, layout=(1,2), size = (1800,600), fg_legend=:transparent, legend=:topright, \n",
    "    bottommargin = 10Plots.mm, leftmargin = 0Plots.mm, ylabel=\"\", framestyle = :box)\n",
    "\n",
    "savefig(\".\\\\\"*outfil*\"_filtered_Spectra\"*Dates.format(start_date, \"_HHMM\")*\".png\")\n",
    "        \n",
    "display(plot_s1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90aaccf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Dates.format(start_date, \"_HHMM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81492a0",
   "metadata": {},
   "source": [
    "## Compare normalised filter against Datawell's DWR4 filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a00cc74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using CSV\n",
    "using StatsBase\n",
    "\n",
    "# Normalise the calculated SavitzkyGolay filter\n",
    "dt1 = fit(UnitRangeTransform, sg.y, dims=1)\n",
    "sg_norm1 = StatsBase.transform(dt1, sg.y)\n",
    "\n",
    "# Read Datawell's DWR4 filter values from file\n",
    "csv_file = \".//Datawell_Mk4_ filter.csv\"\n",
    "DW4_df = DataFrame(CSV.File(csv_file))\n",
    "    \n",
    "dt4 = fit(UnitRangeTransform, DW4_df.Y_val .* -1, dims=1)\n",
    "DWR4_norm = StatsBase.transform(dt4, DW4_df.Y_val .* -1)\n",
    "\n",
    "xpos = times[1]\n",
    "ypos = maximum(displacement)\n",
    "\n",
    "m = length(displacement)\n",
    "sg = savitzky_golay(displacement, window_size, 3);\n",
    "\n",
    "# find where peak in savitzky_golay curve\n",
    "location = findmax(sg.y)[2]\n",
    "\n",
    "# set lower and upper limits of savitzky_golay filter\n",
    "if location-128 > 1\n",
    "    lower = location-128\n",
    "else\n",
    "    lower = 1\n",
    "end\n",
    "\n",
    "if location+128 < length(displacement)\n",
    "    upper = location+128\n",
    "else\n",
    "    upper = length(displacement)\n",
    "end\n",
    "\n",
    "#plot(sg.y, size=(1400,500))\n",
    "plot(-128:128,sg_norm1[lower:upper], marker=:circle, ms=:1, mc=:blue, label=\"Normalised SavitzkyGolay filter\", \n",
    "    fg_legend=:transparent, size=(1400,500), xlim=(-200,200), framestyle = :box)\n",
    "plot!(DW4_df.Point .* 2.56,DWR4_norm, marker=:circle, ms=1, label=\"Normalised DWR4 filter\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00c3b3",
   "metadata": {},
   "source": [
    "### Investigate LombScargle package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2e772",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#import Pkg; Pkg.add(\"LombScargle\")\n",
    "\n",
    "using Plots\n",
    "using LombScargle\n",
    "\n",
    "Sample_frequency = 2.56\n",
    "\n",
    "N = length(heave)\n",
    "t = collect(1:N)/Sample_frequency\n",
    "\n",
    "plan = LombScargle.plan(t, heave, normalization=:psd, fit_mean=:false, center_data=:false, samples_per_peak=:20, maximum_frequency=:0.64);\n",
    "pgram = lombscargle(plan)\n",
    "\n",
    "p = periodogram(heave; onesided=true, nfft=N, fs=2.56)\n",
    "\n",
    "ps_w = welch_pgram(heave, 512, 256; onesided=true, nfft=512, fs=Sample_frequency, window=hanning);\n",
    "f2 = freq(ps_w);\n",
    "Pden2 = power(ps_w);\n",
    "\n",
    "p1 = plot(freqpower(pgram)...,xrange=(0.0,0.64), c=:yellow, lw=:3, label=\"Lomb Scargle\\n\")\n",
    "p1 = plot!(freq(p), power(p), label=\"DSP Periodogram\\n\", c=:blue, lw=:0.5)\n",
    "p1 = plot!(f2,Pden2, label=\"DSP Welch's method\", c=:red, lw=:1)\n",
    "p2 = plot(p1, size=(1200,800), framestyle = :box, fg_legend=:transparent)\n",
    "\n",
    "display(p2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0853852",
   "metadata": {},
   "source": [
    "### Investigate Skewness and Kurtosis calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3122d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using StatsBase, Plots\n",
    "\n",
    "skew = skewness(heave)\n",
    "kurt = kurtosis(heave)\n",
    "\n",
    "#extrema(heave)\n",
    "\n",
    "lim=round(maximum(abs.(extrema(heave))) + 0.2; digits = 1)\n",
    "\n",
    "fit_distribution = fit_mle(Normal,heave)\n",
    "bin_vals = vcat(reverse(collect(-0.2:-0.2:-lim)),collect(0:0.2:lim))\n",
    "        \n",
    "h = histogram(heave, normalize=:false, bins=bin_vals, alpha=0.5)\n",
    "bin_count = fit(Histogram, heave, nbins=length(bin_vals)).weights\n",
    "h = plot!(twinx(), [-lim:0.01:lim],[pdf(fit_distribution,i) for i in -lim:0.01:lim], lc=:blue, lw=:1, ls=:dash)\n",
    "\n",
    "show_normal_plot = plot(h,\n",
    "        framestyle = :box, size = (1400, 800), legend=:topleft, xlim=(first(bin_vals),last(bin_vals)),\n",
    "        leftmargin = 20Plots.mm, rightmargin = 20Plots.mm, fg_legend=:transparent)\n",
    "\n",
    "display(show_normal_plot)\n",
    "\n",
    "bins = collect(collect(fit(Histogram, heave, nbins=length(bins)).edges)[1])\n",
    "println(\"    Bins       Count\")\n",
    "for i in 1:1:length(bin_count)\n",
    "    @printf(\"%4.1f to %4.1f %7d\\n\",bins[i],bins[i+1],bin_count[i])\n",
    "end\n",
    "\n",
    "@printf(\"Skewness = %4.4f\\n\",skew)\n",
    "@printf(\"Kurtosis = %4.4f\\n\",kurt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874aafc",
   "metadata": {},
   "source": [
    "## Show individual WSE's and zero-crossing points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73965b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "found_list = 26 # <<<=== For testing only\n",
    "\n",
    "start_date, start_val, end_val = get_start_end_dates(f23_df,found_list)\n",
    "    \n",
    "# get WSEs for desired 30-minute record\n",
    "heave, north, west = get_hnw(Data,start_val,end_val);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540573a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "zero_up = []; valid_zero_up = []\n",
    "\n",
    "for i in 2:length(heave)-1\n",
    "    if (heave[i]*heave[i+1] < 0 && heave[i+1] > 0) || (heave[i] == 0 && heave[i-1] < 0 && heave[i+1] > 0)\n",
    "        push!(zero_up,i)\n",
    "    end\n",
    "end\n",
    "\n",
    "##med = median(heave)\n",
    "##stdev = std(heave)\n",
    "##stdev_3 = med + stdev*3\n",
    "\n",
    "wse_point = 1:1:length(heave)\n",
    "t = wse_point./2.56\n",
    "\n",
    "wse_1 = plot(t, heave[wse_point], lw=2, c=:blue, alpha=.5, label = \"WSE's\", fillalpha = 0.0075, fillcolor = :blue)\n",
    "wse_1 = scatter!(t, heave[wse_point], c=:white, ms=3, \n",
    "    markerstrokecolor=:blue, alpha=0.5, markerstrokewidth=0.5, label=\"WSE points\")\n",
    "wse_1 = scatter!(zero_up[1:24]./2.56, heave[zero_up[1:24]], ms=4, c=:lightgreen, xlim=(1,20), \n",
    "    markerstrokecolor=:green, alpha=0.5, series_annotations = text.(zero_up, :bottom, :red, :size, 10), \n",
    "    annotationhalign = :hcenter, label=\"Zero up-cross points\")\n",
    "\n",
    "# heave Threshold set at 10mm. Refer to Section 9 Wave statistics pp. 9-10 in Datawell Library Manual\n",
    "threshold = 0.05\n",
    "\n",
    "wse_1 = hline!([threshold; threshold], lw=0.2, ls =:dot, c=:red, label=\"Threshold (\"*L\"\\pm\"*string(threshold)*\")\\n\")\n",
    "wse_1 = hline!([-threshold; -threshold], lw=0.2, ls =:dot, c=:red, label=\"\")\n",
    "\n",
    "##wse_1 = hline!([stdev_3; stdev_3], lw=0.2, ls =:dot, c=:green, label=\"3 sigma\")\n",
    "##wse_1 = hline!([-stdev_3; -stdev_3], lw=0.2, ls =:dot,  c=:green, label=\"\")\n",
    "\n",
    "##wse_plot = plot(wse_point,wse_1, size = (1400, 600),xlim=(length(heave)-100,length(heave)), ylim=(-1.5,1.5), framestyle = :box, \n",
    "wse_plot = plot(wse_1, size = (1400, 600),xlim=(1000,1200), xlabel=\"Time (s)\", ylabel=\"Displacement (m)\", \n",
    "#    ylim=(-1.5,1.5), \n",
    "    framestyle = :box,     \n",
    "    fg_legend=:transparent, bg_legend=:transparent, legend=:topright,\n",
    "    margin = 15Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, show=true)\n",
    "\n",
    "#save plot\n",
    "savefig(\".\\\\plot.png\")\n",
    "\n",
    "display(wse_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713761e5",
   "metadata": {},
   "source": [
    "## Identify individual waves and calculate time-domain heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c25335",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "valid_zero_up = []; crest_points = []; trough_points = []\n",
    "i = 1; j = 2\n",
    "\n",
    "while j < length(zero_up)-1\n",
    "    \n",
    "    crest = maximum(heave[zero_up[i]:zero_up[j]])\n",
    "    crest_point = zero_up[i] + argmax(heave[zero_up[i]:zero_up[j]]) - 1\n",
    "    trough = minimum(heave[crest_point:zero_up[j]])\n",
    "\n",
    "    # Check that crest higher than threshold AND trough less than threshold - Possible Valid Wave!!\n",
    "    if (crest > threshold) & (trough < -threshold)\n",
    "        crest_point = zero_up[i] + argmax(heave[zero_up[i]:zero_up[j]]) - 1\n",
    "        trough_point = crest_point + argmin(heave[crest_point:zero_up[j]]) - 1\n",
    "        \n",
    "        push!(crest_points,crest_point)\n",
    "        push!(trough_points,trough_point)\n",
    "        \n",
    "        next_crest = maximum(heave[zero_up[j]:zero_up[j+1]])\n",
    "        \n",
    "        # Check that NEXT crest also exceeds threshold (if so then Valid Wave)\n",
    "        if (next_crest > threshold)\n",
    "##            println(\"Crest found at \",crest_point,\" Trough at \",trough_point)\n",
    "            push!(valid_zero_up,(zero_up[i],zero_up[j]));\n",
    "            i = j\n",
    "        end\n",
    "        \n",
    "    end\n",
    "\n",
    "    j = j+1\n",
    "    \n",
    "end\n",
    "\n",
    "# Process last recorded wave\n",
    "#i = j\n",
    "#j = j+1\n",
    "\n",
    "crest = maximum(heave[zero_up[i]:zero_up[j]])\n",
    "trough = minimum(heave[zero_up[i]:zero_up[j]])\n",
    "\n",
    "if (crest > threshold) & (trough < -threshold)\n",
    "\n",
    "    crest_point = zero_up[i] + argmax(heave[zero_up[i]:zero_up[j]]) - 1\n",
    "    trough_point = crest_point + argmin(heave[crest_point:zero_up[j]]) - 1\n",
    "    push!(valid_zero_up,(zero_up[i],zero_up[j]));\n",
    "\n",
    "end\n",
    "\n",
    "heights = []\n",
    "\n",
    "for i in 1:length(valid_zero_up)\n",
    "    \n",
    "    crest = maximum(heave[valid_zero_up[i][1]:valid_zero_up[i][2]]);\n",
    "    trough = minimum(heave[valid_zero_up[i][1]:valid_zero_up[i][2]]);\n",
    "    push!(heights,crest - trough)\n",
    "##    @printf(\"Wave %d = %2.3f\\n\",i,crest - trough)\n",
    "\n",
    "end \n",
    "\n",
    "# Get time-domain height parameters\n",
    "sorted_heights = sort(heights, rev=true) # sort heights in reverse order heighestwave to loheave wave\n",
    "hmax = maximum(sorted_heights)\n",
    "hs = mean(sorted_heights[1:Int(ceil(length(sorted_heights)/3))])\n",
    "h10 = mean(sorted_heights[1:Int(ceil(length(sorted_heights) / 10))])\n",
    "hmean = mean(sorted_heights)\n",
    "\n",
    "@printf(\"%s; Waves = %3d; Hmean = %4.2fm; Hs = %4.2fm; H10 = %4.2fm; Hmax = %4.2fm\\n\",Dates.format(start_date, \"yyyy-mm-dd HH:MM\"),length(heights), hmean, hs, h10, hmax)\n",
    "\n",
    "## Locate the zero-crossing points\n",
    "\n",
    "x_point = []\n",
    "for i in 1:length(valid_zero_up)\n",
    "    push!(x_point,valid_zero_up[i][1] + abs(heave[valid_zero_up[i][1]]) / (heave[valid_zero_up[i][1]+1] - heave[valid_zero_up[i][1]]))\n",
    "end\n",
    "\n",
    "# Process final zero-crossing point\n",
    "i = length(valid_zero_up)\n",
    "push!(x_point,valid_zero_up[i][2] + abs(heave[valid_zero_up[i][2]]) / (heave[valid_zero_up[i][2]+1] - heave[valid_zero_up[i][2]]))\n",
    "\n",
    "ix = 15\n",
    "\n",
    "# Do plots\n",
    "wse_1 = plot(wse_point./2.56, heave[wse_point], lw=2, c=:blue, alpha=0.5, label = \"WSE's\", fillrange = 0, fillalpha = 0.0075, fillcolor = :blue)\n",
    "wse_1 = scatter!(wse_point./2.56, heave[wse_point], c=:white, ms=3, \n",
    "    markerstrokecolor=:blue, alpha=0.5, markerstrokewidth=0.5,label=\"WSE points\")\n",
    "wse_1 = scatter!(zero_up[1:ix]./2.56, heave[zero_up[1:ix]], ms=3, c=:lightgreen, \n",
    "    markerstrokecolor=:lightgreen, series_annotations = text.(zero_up, :bottom, :red, :size, 10), \n",
    "    annotationhalign = :hcenter, label=\"Zero up-cross points\\n\")\n",
    "\n",
    "# heave Threshold set at 10mm. Refer to Section 9 Wave statistics pp. 9-10 in Datawell Library Manual\n",
    "threshold = 0.05 \n",
    "\n",
    "wse_1 = hline!([threshold; threshold], lw=0.2, ls =:dot, c=:red, label=\"Threshold\\n\")\n",
    "wse_1 = hline!([-threshold; -threshold], lw=0.2, ls =:dot, c=:red, label=\"\")\n",
    "\n",
    "wse_1 = scatter!(x_point./2.56, zeros(length(x_point)), c=:yellow, ms=5, \n",
    "    markerstrokecolor=:yellow, markershape=:diamond, label=\"Zero-crossing points\")\n",
    "\n",
    "\n",
    "#wse_plot = plot(wse_1, size = (1400, 800),xlim=(length(heave)-100,length(heave)), ylim=(-1.5,1.5), framestyle = :box, \n",
    "wse_plot = plot(wse_1, size = (1400, 600), xlabel=\"Time (s)\", ylabel=\"Displacement (m)\", xlim=(20,60), ylim=(-1.5,1.5), framestyle = :box,     \n",
    "    fg_legend=:transparent, bg_legend=:transparent, legend=:topright,\n",
    "    margin = 15Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, show=true)\n",
    "\n",
    "#save plot\n",
    "savefig(\".\\\\plot.png\")\n",
    "\n",
    "display(wse_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d084a1",
   "metadata": {},
   "source": [
    "## Calculate time-domain periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ee84b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "params_df = DataFrame(Date = [], Hmax = [], Thmax = [], Tmax = [], Htmax = [], Havg = [], \n",
    "    Tavg = [], Hsrms = [], Nw = [], Nc = [], Epsilon = [], Coverage = [])\n",
    "sample_frequency = 2.56 # Hertz Mk4\n",
    "periods = []\n",
    "\n",
    "for i in 1:length(x_point)-1\n",
    "    push!(periods,(x_point[i+1]-x_point[i]) / sample_frequency) # wave period in seconds\n",
    "end\n",
    "\n",
    "sorted_periods = sort(periods, rev=true) # sort periods in reverse order longest period to shortest period\n",
    "tmean = mean(sorted_periods)\n",
    "ths = periods[argmin(abs.(heights .- hs))] \n",
    "th10 = periods[argmin(abs.(heights .- h10))]\n",
    "thmax = periods[argmax(heights)]\n",
    "tmax = maximum(sorted_periods)\n",
    "\n",
    "# get Datawell parameters from f29_df\n",
    "row = f29_df[f29_df.Date .== start_date + Minute(30), :]\n",
    "\n",
    "# Print results\n",
    "@printf(\"\\nQGHL values:     \")\n",
    "@printf(\"%s; Waves = %3d; Hmean = %5.2fm; Hs = %5.2fm; H10 = %5.2fm; Hmax = %5.2fm;\",Dates.format(start_date, \"yyyy-mm-dd HH:MM\"),length(heights), hmean, hs, h10, hmax)\n",
    "@printf(\" Tmean = %5.2fs; THs = %5.2fs; TH10 = %5.2fs; THmax = %5.2fs; Tmax = %5.2fs\",tmean,ths,th10,thmax,tmax)\n",
    "\n",
    "@printf(\"\\nDatawell values: \")\n",
    "@printf(\"%s; Waves = %3d; Hmean = %5.2fm; Hs = %5.2fm; H10 = %5.2fm; Hmax = %5.2fm;\",Dates.format(row.Date[1], \"yyyy-mm-dd HH:MM\"),row.Nw[1], row.Havg[1], row.H3[1], row.H10[1], row.Hmax[1])\n",
    "@printf(\" Tmean = %5.2fs; THs = %5.2fs; TH10 = %5.2fs; THmax = %5.2fs\\n\",row.Tavg[1],row.TH3[1],row.TH10[1],row.THmax[1])\n",
    "\n",
    "\n",
    "## Plot wave heights and periods\n",
    "\n",
    "title_string = Dates.format(start_date, \"dd/mm/yyyy HH:MM\")\n",
    "\n",
    "wave_heights = hline([hmax; hmax], lw=1, c=:red, label=\"\")\n",
    "wave_heights = hline!([h10; h10], lw=1.5, c=:pink, label=\"\")\n",
    "wave_heights = hline!([hs; hs], lw=1, c=:blue, label=\"\")\n",
    "wave_heights = hline!([hmean; hmean], lw=1.5, c=:green, label=\"\")\n",
    "\n",
    "wave_heights = annotate!([15], [hmax*1.05], text(\"Hmax \" * string(round(hmax, digits=2)) * \"m\", :red, 10), :top)\n",
    "wave_heights = annotate!([15], [h10*1.05], text(\"H10 \" * string(round(h10, digits=2)) * \"m\", :pink, 10), :top)\n",
    "wave_heights = annotate!([15], [hs*1.05], text(\"Hs \" * string(round(hs, digits=2)) * \"m\", :blue, 10), :top)\n",
    "wave_heights = annotate!([15], [hmean*1.05], text(\"Hmean \" * string(round(hmean, digits=2)) * \"m\", :green, 10), :top)\n",
    "\n",
    "\n",
    "wave_heights = plot!(heights, lw=2, c=:\"#4a536b\", fillrange = 0, fillalpha = 0.035, fillcolor = :\"#4a536b\",\n",
    "    ylim=(0,hmax*1.1), title=title_string, ylabel=\"Wave height (m)\", label=\"\")\n",
    "\n",
    "min_h3_wave = minimum(sorted_heights[1:Int(ceil(length(sorted_heights)/3))])\n",
    "# show which waves were used in calculation of Hs\n",
    "wave_heights = scatter!(findall(i->(i>=min_h3_wave), heights),heights[findall(i->(i>=min_h3_wave), heights)], ms=4, mc=:lightblue, ma=0.25, label=\"\")\n",
    "\n",
    "wave_periods = hline([tmax; tmax], lw=1, c=:red, label=\"\")\n",
    "wave_periods = hline!([thmax; thmax], lw=1.5, c=:pink, label=\"\")\n",
    "#wave_periods = hline!([ths; ths], lw=1, c=:blue, label=\"\")\n",
    "\n",
    "wave_periods = annotate!([15], [tmax*1.05], text(\"Tmax \" * string(round(tmax, digits=2)) * \"s\", :red, 10), :top)\n",
    "#wave_periods = annotate!([15], [thmax*1.05], text(\"THmax \" * string(round(thmax, digits=2)) * \"s\", :pink, 10), :top)\n",
    "#wave_periods = annotate!([15], [ths*1.05], text(\"THs \" * string(round(ths, digits=2)) * \"s\", :blue, 10), :top)\n",
    "\n",
    "wave_periods = plot!(periods, lw=2, c=:red, fillrange = 0, fillalpha = 0.025, fillcolor = :red, label=\"\",\n",
    "    ylim=(0,tmax*1.1), xlabel=\"Wave number\", ylabel=\"Wave period (s)\")\n",
    "\n",
    "wave_heights_plot = plot(wave_heights, wave_periods, layout = (2, 1), size = (1400, 800), xlim=(0,length(heights)*1.015),\n",
    "    framestyle = :box, titlefontsize=12, fg_legend=:transparent, bg_legend=:transparent,\n",
    "    leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, show=true)\n",
    "\n",
    "display(wave_heights_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd98b84",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using Plots\n",
    "\n",
    "function SmoothedZscoreAlgo(y, lag, threshold, influence)\n",
    "################################################    \n",
    "    # Julia implimentation of http://stackoverflow.com/a/22640362/6029703\n",
    "    \n",
    "    n = length(y)\n",
    "    signals = zeros(n) # init signal results\n",
    "    filteredY = copy(y) # init filtered series\n",
    "    avgFilter = zeros(n) # init average filter\n",
    "    stdFilter = zeros(n) # init std filter\n",
    "    avgFilter[lag - 1] = mean(y[1:lag]) # init first value\n",
    "    stdFilter[lag - 1] = std(y[1:lag]) # init first value\n",
    "\n",
    "    for i in range(lag, stop=n-1)\n",
    "        if abs(y[i] - avgFilter[i-1]) > threshold*stdFilter[i-1]\n",
    "            if y[i] > avgFilter[i-1]\n",
    "                signals[i] += 1 # postive signal\n",
    "            else\n",
    "                signals[i] += -1 # negative signal\n",
    "            end\n",
    "            # Make influence lower\n",
    "            filteredY[i] = influence*y[i] + (1-influence)*filteredY[i-1]\n",
    "        else\n",
    "            signals[i] = 0\n",
    "            filteredY[i] = y[i]\n",
    "        end\n",
    "        avgFilter[i] = mean(filteredY[i-lag+1:i])\n",
    "        stdFilter[i] = std(filteredY[i-lag+1:i])\n",
    "    end\n",
    "    \n",
    "    return (signals = signals, avgFilter = avgFilter, stdFilter = stdFilter)\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "# Data\n",
    "y = periods\n",
    "\n",
    "# Settings: lag = 30, threshold = 5, influence = 3\n",
    "lag = 30\n",
    "threshold = 5 #std(y)*3\n",
    "influence = .3\n",
    "\n",
    "results = SmoothedZscoreAlgo(y, lag, threshold, influence)\n",
    "upper_bound = results[:avgFilter] + threshold * results[:stdFilter]\n",
    "lower_bound = results[:avgFilter] - threshold * results[:stdFilter]\n",
    "x = 1:length(y)\n",
    "\n",
    "yplot = plot(x, y, color=:blue, label=\"Wave period\\n\", legend=:topleft, fg_legend=:transparent, bg_legend=:transparent)\n",
    "#yplot = plot!(x,upper_bound, color=:green, label=\"Upper Bound\")\n",
    "yplot = plot!(x,results[:avgFilter], color=:cyan, lw=:3, label=\"Average Filter\")\n",
    "#yplot = plot!(x,lower_bound, color=:green, label=\"Lower Bound\")\n",
    "\n",
    "subplot = Plots.twinx()\n",
    "\n",
    "yplot = plot!(subplot, x, results[:signals], color=:red, label=\"Suspects\", legend=:topright, fg_legend=:transparent, bg_legend=:transparent)\n",
    "\n",
    "plot(yplot,size = (1800, 500), framestyle = :box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5fa795",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mean(y) + std(y)*3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db82e32",
   "metadata": {},
   "source": [
    "## Plot normal distribution on WSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eacc0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "global title_string = \"JJ\"\n",
    "plot_normal(heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe7190",
   "metadata": {},
   "source": [
    "## Do Rayleigh plot of wave heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcff183",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_rayleigh(heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77e7c0",
   "metadata": {},
   "source": [
    "## Calculate declination and Inclination for record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294d872",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "##############################################################################################\n",
    "## Using model located at http://www.geomag.bgs.ac.uk/data_service/models_compass/wmm_calc.html\n",
    "## But different values from https://www.ngdc.noaa.gov/geomag/calculators/magcalc.shtml#igrfgrid\n",
    "##############################################################################################\n",
    "##############################################################################################\n",
    "\n",
    "using XLSX\n",
    "using CategoricalArrays\n",
    "\n",
    "function get_lower_and_upper(year_dec_df, year_inc_df, lower_lat, lower_lon, upper_lat, upper_lon)\n",
    "################################################    \n",
    "\n",
    "    # get the declinations for lower latitude\n",
    "    lower_lat_lower_lon_declination = year_dec_df[findfirst(year_dec_df.Lat .== lower_lat),string(lower_lon)]\n",
    "##    println(\"Lower Lat & Lower Lon \", lower_lat,' ',lower_lon,' ' ,lower_lat_lower_lon_declination)\n",
    "    lower_lat_upper_lon_declination = year_dec_df[findfirst(year_dec_df.Lat .== lower_lat),string(upper_lon)]\n",
    "##    println(\"Lower Lat & Upper Lon \", lower_lat,' ',upper_lon,' ' ,lower_lat_upper_lon_declination)\n",
    "\n",
    "    # get the declinations for upper latitude\n",
    "    upper_lat_lower_lon_declination = year_dec_df[findfirst(year_dec_df.Lat .== upper_lat),string(lower_lon)]\n",
    "##    println(\"Upper Lat & Lower Lon \", upper_lat,' ',lower_lon,' ' ,upper_lat_lower_lon_declination)\n",
    "    upper_lat_upper_lon_declination = year_dec_df[findfirst(year_dec_df.Lat .== upper_lat),string(upper_lon)]\n",
    "##    println(\"Upper Lat & Upper Lon \", upper_lat,' ',upper_lon,' ' ,upper_lat_upper_lon_declination)\n",
    "\n",
    "    # get the inclinations for lower latitude\n",
    "    lower_lat_lower_lon_inclination = year_inc_df[findfirst(year_inc_df.Lat .== lower_lat),string(lower_lon)]\n",
    "##    println(\"Lower Lat & Lower Lon \", lower_lat,' ',lower_lon,' ' ,lower_lat_lower_lon_inclination)\n",
    "    lower_lat_upper_lon_inclination = year_inc_df[findfirst(year_inc_df.Lat .== lower_lat),string(upper_lon)]\n",
    "##    println(\"Lower Lat & Upper Lon \", lower_lat,' ',upper_lon,' ' ,lower_lat_upper_lon_inclination)\n",
    "\n",
    "    # get the inclinations for upper latitude\n",
    "    upper_lat_lower_lon_inclination = year_inc_df[findfirst(year_inc_df.Lat .== upper_lat),string(lower_lon)]\n",
    "##    println(\"Upper Lat & Lower Lon \", upper_lat,' ',lower_lon,' ' ,upper_lat_lower_lon_inclination)\n",
    "    upper_lat_upper_lon_inclination = year_inc_df[findfirst(year_inc_df.Lat .== upper_lat),string(upper_lon)]\n",
    "##    println(\"Upper Lat & Upper Lon \", upper_lat,' ',upper_lon,' ' ,upper_lat_upper_lon_inclination)\n",
    "    \n",
    "    return(lower_lat_lower_lon_declination, lower_lat_upper_lon_declination, upper_lat_upper_lon_declination, upper_lat_lower_lon_declination, \n",
    "                lower_lat_lower_lon_inclination, lower_lat_upper_lon_inclination, upper_lat_upper_lon_inclination, upper_lat_lower_lon_inclination)\n",
    "    \n",
    "end    # get_lower_and_upper()\n",
    "    \n",
    "\n",
    "function get_point_value(lat_diff, lon_diff, upper_lower, lower_lower, upper_upper, lower_upper)\n",
    "################################################    \n",
    "# function to calculate declination or inclination of point located with grid cell\n",
    "    \n",
    "    # first using latitude differences\n",
    "    lower_diff = upper_lower - lower_lower\n",
    "    upper_diff = upper_upper - lower_upper\n",
    "\n",
    "##    println(lower_diff,' ',upper_diff)\n",
    "\n",
    "    lower_record = lower_lower - lower_diff*lat_diff\n",
    "    upper_record = lower_upper - upper_diff*lat_diff\n",
    "\n",
    "    lon_record_using_latitude = lower_record + (upper_record-lower_record)*lon_diff\n",
    "\n",
    "##    println(lower_record,' ',upper_record,' ',lon_record_using_latitude)\n",
    "    \n",
    "    # second using longitude differences\n",
    "    lower_diff = lower_upper - lower_lower\n",
    "    upper_diff = upper_upper - upper_lower\n",
    "\n",
    "##    println(lower_diff,' ',upper_diff)\n",
    "\n",
    "    lower_record = lower_lower + lower_diff*lon_diff\n",
    "    upper_record = upper_lower + upper_diff*lon_diff\n",
    "\n",
    "    lon_record_using_longitude = lower_record - (upper_record-lower_record)*lat_diff\n",
    "\n",
    "##    println(lower_record,' ',upper_record,' ',lon_record_using_longitude)\n",
    "    \n",
    "    # return mean value of the latitude and longitude calculations\n",
    "    return(mean([lon_record_using_longitude,lon_record_using_latitude]))\n",
    "    \n",
    "end    # get_point_value()\n",
    "\n",
    "\n",
    "# Get approximate date of record\n",
    "start_year = mode(Dates.year.(f80_df.Date))\n",
    "end_year = start_year + 1\n",
    "record_month = mode(Dates.month.(f80_df.Date))\n",
    "\n",
    "@printf(\"Calculation date = 01/%2.2d/%4.4d using World Magnetic Model\\n\", record_month, start_year)\n",
    "# Determine first worksheet number\n",
    "start_sheet = indexin(start_year,[2023, 2024, 2025])[1] + 1\n",
    "end_sheet = start_sheet + 1\n",
    "\n",
    "# Read Declination and Inclination values from file\n",
    "excel_directory = \".\\\\\" \n",
    "excel_file = excel_directory*\"Declination_and_Inclination_2023_2024.xlsx\"\n",
    "buoys_df = DataFrame(XLSX.readtable(excel_file,1,))\n",
    "start_dec_df = DataFrame(XLSX.readtable(excel_file,start_sheet,))\n",
    "end_dec_df = DataFrame(XLSX.readtable(excel_file,end_sheet,))\n",
    "                \n",
    "start_inc_df = DataFrame(XLSX.readtable(excel_file,start_sheet+3,))\n",
    "end_inc_df = DataFrame(XLSX.readtable(excel_file,end_sheet+3,));\n",
    "\n",
    "# Get a representative Lat and Lon for the record\n",
    "record_lat = (mode(f80_df.Latitude))\n",
    "record_lon = mode(f80_df.Longitude);\n",
    "@printf(\"Buoy position from GPS: %4.5f° %4.5f°\\n\", record_lat, record_lon)\n",
    "\n",
    "#get integer latitudes above and below record latitude\n",
    "lower_lat = trunc(Int, record_lat)\n",
    "upper_lat = lower_lat - 1\n",
    "\n",
    "#get integer longitudes above and below record longitude\n",
    "lower_lon = trunc(Int, record_lon)\n",
    "upper_lon = lower_lon + 1\n",
    "\n",
    "\n",
    "start_lower_lat_lower_lon_declination, start_lower_lat_upper_lon_declination, start_upper_lat_upper_lon_declination, start_upper_lat_lower_lon_declination, \n",
    "    start_lower_lat_lower_lon_inclination, start_lower_lat_upper_lon_inclination, start_upper_lat_upper_lon_inclination, start_upper_lat_lower_lon_inclination =\n",
    "        get_lower_and_upper(start_dec_df, start_inc_df, lower_lat, lower_lon, upper_lat, upper_lon)\n",
    "\n",
    "end_lower_lat_lower_lon_declination, end_lower_lat_upper_lon_declination, end_upper_lat_upper_lon_declination, end_upper_lat_lower_lon_declination, \n",
    "    end_lower_lat_lower_lon_inclination, end_lower_lat_upper_lon_inclination, end_upper_lat_upper_lon_inclination, end_upper_lat_lower_lon_inclination =\n",
    "        get_lower_and_upper(end_dec_df, end_inc_df, lower_lat, lower_lon, upper_lat, upper_lon)\n",
    "\n",
    "lat_diff = record_lat - lower_lat\n",
    "lon_diff = record_lon - lower_lon\n",
    "\n",
    "# calculate declination and inclination of records position at start of year\n",
    "start_declination = get_point_value(lat_diff, lon_diff, start_upper_lat_lower_lon_declination, start_lower_lat_lower_lon_declination, start_upper_lat_upper_lon_declination, start_lower_lat_upper_lon_declination)\n",
    "start_inclination = get_point_value(lat_diff, lon_diff, start_upper_lat_lower_lon_inclination, start_lower_lat_lower_lon_inclination, start_upper_lat_upper_lon_inclination, start_lower_lat_upper_lon_inclination)\n",
    "\n",
    "# calculate declination and inclination of records position at end of year\n",
    "end_declination = get_point_value(lat_diff, lon_diff, end_upper_lat_lower_lon_declination, end_lower_lat_lower_lon_declination, end_upper_lat_upper_lon_declination, end_lower_lat_upper_lon_declination)\n",
    "end_inclination = get_point_value(lat_diff, lon_diff, end_upper_lat_lower_lon_inclination, end_lower_lat_lower_lon_inclination, end_upper_lat_upper_lon_inclination, end_lower_lat_upper_lon_inclination)\n",
    "\n",
    "@printf(\"01/01/%4d Declination = %4.5f; Inclination = %4.5f\\n\", start_year, start_declination, start_inclination)\n",
    "@printf(\"01/01/%4d Declination = %4.5f; Inclination = %4.5f\\n\", end_year, end_declination, end_inclination)\n",
    "\n",
    "# get the declination and inclination using start date + month / 12\n",
    "declination = start_declination + (end_declination-start_declination)*record_month/12\n",
    "inclination = start_inclination + (end_inclination-start_inclination)*record_month/12\n",
    "@printf(\"01/%2.2d/%4d Declination = %4.5f; Inclination = %4.5f\\n\", record_month, start_year, declination, inclination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b99a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Compute the estimated values of magnetic declination and inclination based on the current World Magnetic Model (WMM) or the International Geomagnetic Reference Field (IGRF) model.\n",
    "using XLSX\n",
    "using CategoricalArrays\n",
    "\n",
    "# Read Declination and Inclination values from file\n",
    "excel_directory = \".\\\\\" \n",
    "\n",
    "# Select \n",
    "##excel_file = excel_directory*\"IGRF_13_Declination_and_Inclination.xlsx\"\n",
    "excel_file = excel_directory*\"WMM_2020_2024_Declination_and_Inclination.xlsx\"\n",
    "\n",
    "dec_df = DataFrame(XLSX.readtable(excel_file,1,))\n",
    "inc_df = DataFrame(XLSX.readtable(excel_file,2,));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0cb9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get the declination and inclination of the four points bounding the record position\n",
    "dec_box = dec_df[(dec_df.Year .== start_year) .&& (upper_lat .<= dec_df.Latitude .<= lower_lat) .&& (lower_lon .<= dec_df.Longitude .<= upper_lon),:]\n",
    "#inc_box = inc_df[(inc_df.Year .== start_year) .&& (upper_lat .<= inc_df.Latitude .<= lower_lat) .&& (lower_lon .<= inc_df.Longitude .<= upper_lon),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70ad84",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get the declination and inclination of the four points bounding the record position\n",
    "dec_box = dec_df[(dec_df.Year .== start_year) .&& (upper_lat .<= dec_df.Latitude .<= lower_lat) .&& (lower_lon .<= dec_df.Longitude .<= upper_lon),:]\n",
    "inc_box = inc_df[(inc_df.Year .== start_year) .&& (upper_lat .<= inc_df.Latitude .<= lower_lat) .&& (lower_lon .<= inc_df.Longitude .<= upper_lon),:]\n",
    "\n",
    "dec_vals = dec_box.Declination\n",
    "#inc_vals = inc_box.Inclination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f90e08",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dec_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e478897",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inc_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c95572d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dec_box = dec_df[(dec_df.Year .== start_year) .&& (upper_lat .<= dec_df.Latitude .<= lower_lat) .&& (lower_lon .<= dec_df.Longitude .<= upper_lon),:]\n",
    "inc_box = inc_df[(inc_df.Year .== start_year) .&& (upper_lat .<= inc_df.Latitude .<= lower_lat) .&& (lower_lon .<= inc_df.Longitude .<= upper_lon),:]\n",
    "dec_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f9063",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "get_point_value(lat_diff, lon_diff, start_upper_lat_lower_lon_declination, start_lower_lat_lower_lon_declination, start_upper_lat_upper_lon_declination, start_lower_lat_upper_lon_declination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f2f21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "aa = dec_box.Declination\n",
    "bb = inc_box.Inclination\n",
    "\n",
    "get_point_value(lat_diff, lon_diff, aa[2], aa[1], aa[4], aa[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d857fd4",
   "metadata": {},
   "source": [
    "## How to create a 30-minute array of times incrementing by 2.56 Hertz (Mk4 sample frequency)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef860f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "julian2datetime.(datetime2julian(start_date) .+ collect(0:1/2.56:1800) ./ 86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad68d9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Dates.format(start_date, \"HH:MM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb00e5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef84d7f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using CSV\n",
    "using Dates, DataFrames, Distributions, DSP\n",
    "#using Gtk\n",
    "using LaTeXStrings\n",
    "using NativeFileDialog\n",
    "using Plots, Printf, PyPlot\n",
    "using Statistics #, StatsPlots\n",
    "#using Tk\n",
    "\n",
    "\n",
    "function do_subplot(x, y, z)\n",
    "############################    \n",
    "    ax = subplot(x, y, z, polar=\"true\") \n",
    "    ax.grid(:true, fontsize=:10)\n",
    "    ax.set_rlabel_position(-90)\n",
    "    ax.set_theta_zero_location(\"N\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    \n",
    "    PyPlot.title(string(z))\n",
    "\n",
    "end    # do_subplot()\n",
    "    \n",
    "\n",
    "function plot_polar(fig, displacement_df, row, y, z, spec_max)\n",
    "##############################################################            \n",
    "##    println(row,' ',y,' ',z)\n",
    "\n",
    "    x = row\n",
    "    Chh = displacement_df.Chh[z]\n",
    "    a1 = displacement_df.a1[z]\n",
    "    b1 = displacement_df.b1[z] \n",
    "    a2 = displacement_df.a2[z] \n",
    "    b2 = displacement_df.b2[z]\n",
    "    time_string  = displacement_df.Time_string[z]\n",
    "\n",
    "    aa = length(Chh)\n",
    "\n",
    "    r = 1:aa\n",
    "    ρ = r ./ (aa/nyquist) \n",
    "\n",
    "    θ = 1:1:361\n",
    "\n",
    "    fx(r, θ) =  max(Chh[r] * (a1[r]*cos(θ) + b1[r]*sin(θ) + a2[r]*cos(2θ) + b2[r]*sin(2θ)),0)\n",
    "    ax = fig.add_subplot(x, y, z, projection=\"polar\")\n",
    "    p1 = ax.set_title(time_string)\n",
    "    p1 = ax.set_theta_zero_location(\"N\")\n",
    "    p1 = ax.set_theta_direction(-1)\n",
    "    p1 = ax.set_ylim(0,0.4)\n",
    "\n",
    "    p1 = ax.contourf(θ, ρ, fx.(r, θ'), alpha=0.95, levels=20, antialiased =:false, extend=\"both\", vmin=0, vmax=spec_max, cmap=\"Spectral_r\")\n",
    "\n",
    "#    cbar = PyPlot.plt.colorbar(p1, aspect=:50, shrink=:0.75, spacing=\"proportional\", pad=:0.1)\n",
    "#    cbar.set_label(\"Spectral Density (m²/Hz.)\")\n",
    "\n",
    "    rlabels = ax.get_ymajorticklabels()\n",
    "\n",
    "    for label in rlabels\n",
    "        label.set_color(\"white\"), label.set_fontsize(10)\n",
    "    end\n",
    "\n",
    "    p1 = ax[:grid](:true, color=:white, ls=:dotted)\n",
    "    \n",
    "end    # plot_polar()\n",
    "    \n",
    "\n",
    "function get_Fourier_coefficients(heave, north, west)\n",
    "#####################################################    \n",
    "    # Get the cross periodograms\n",
    "    cps_heave_heave = mt_cross_power_spectra([heave heave]', fs=sample_frequency);\n",
    "    cps_north_north = mt_cross_power_spectra([north north]', fs=sample_frequency);\n",
    "    cps_west_west = mt_cross_power_spectra([west west]', fs=sample_frequency);\n",
    "\n",
    "    cps_north_heave = mt_cross_power_spectra([north heave]', fs=sample_frequency);\n",
    "    cps_west_heave = mt_cross_power_spectra([west heave]', fs=sample_frequency);\n",
    "    cps_north_west = mt_cross_power_spectra([north west]', fs=sample_frequency);\n",
    "\n",
    "    fhh = cps_heave_heave.freq\n",
    "    Chh = real.(cps_heave_heave.power[1,1,:])\n",
    "\n",
    "    #fnn = cps_north_north.freq\n",
    "    Cnn = real.(cps_north_north.power[1,1,:])\n",
    "\n",
    "    #fww = cps_west_west.freq\n",
    "    Cww = real.(cps_west_west.power[1,1,:])\n",
    "\n",
    "    #fnw = cps_north_west.freq\n",
    "    Cnw = real.(cps_north_west.power[1,2,:])\n",
    "\n",
    "    #fnh = cps_north_heave.freq\n",
    "    Qnh = imag.(cps_north_heave.power[1,2,:])\n",
    "\n",
    "    #fwh = cps_west_heave.freq\n",
    "    Qwh = imag.(cps_west_heave.power[1,2,:])\n",
    "\n",
    "    a1 = Qnh ./ ((Cnn .+ Cww) .* Chh) .^ 0.5\n",
    "    b1 = -Qwh ./ ((Cnn .+ Cww) .* Chh) .^ 0.5\n",
    "\n",
    "    a2 = (Cnn .- Cww) ./ (Cnn .+ Cww)\n",
    "    b2 = -2 .* Cnw ./ (Cnn .+ Cww)\n",
    "    \n",
    "    return(fhh, Chh, a1, b1, a2, b2)\n",
    "    \n",
    "end    # get_Fourier_coefficients()\n",
    "\n",
    "\n",
    "function get_displacements(arry)\n",
    "#####################################\n",
    "    \n",
    "    displacements = []\n",
    "\n",
    "    if length(arry[1]) == 3\n",
    "    \n",
    "        for i in arry\n",
    "            append!(displacements,parse(Int, SubString.(i, 1, 1), base=16)*16^2 + parse(Int, SubString.(i, 2, 2), base=16)*16^1 + parse(Int, SubString.(i, 3, 3), base=16)*16^0)\n",
    "        end\n",
    "        \n",
    "    else\n",
    "        \n",
    "        for i in arry\n",
    "            append!(displacements,parse(Int, SubString.(i, 1, 1), base=16)*16^1 + parse(Int, SubString.(i, 2, 2), base=16)*16^0)\n",
    "        end\n",
    "        \n",
    "    end\n",
    "\n",
    "    displacements[findall(>=(2048), displacements)] = 2048 .- displacements[findall(>=(2048), displacements)];\n",
    "    \n",
    "    return(displacements./100)\n",
    "    \n",
    "end     # get_displacements()\n",
    "\n",
    "\n",
    "function get_HNW(infil)\n",
    "#####################################\n",
    "        \n",
    "    global df = DataFrame(CSV.File(infil,header=0, delim=\",\", types=String));\n",
    "\n",
    "    # Calculate sequence numbers\n",
    "    arry = SubString.(df.Column1, 3, 4)\n",
    "\n",
    "    global sequence = []\n",
    "\n",
    "    for i in arry\n",
    "        append!(sequence,parse(Int, SubString.(i, 1, 1), base=16)*16^1 + parse(Int, SubString.(i, 2, 2), base=16)*16^0)\n",
    "    end\n",
    "\n",
    "    # Calculate heave WSEs\n",
    "    arry = SubString.(df.Column3, 1, 3);\n",
    "    heave = get_displacements(arry);\n",
    "\n",
    "    # Calculate north WSEs\n",
    "    arry = SubString.(df.Column3, 4, ) .* SubString.(df.Column4, 1, 2)\n",
    "    north = get_displacements(arry);\n",
    "\n",
    "    # Calculate north WSEs\n",
    "    arry = SubString.(df.Column4, 3, 4) .* SubString.(df.Column5, 1, 1)\n",
    "    west = get_displacements(arry);\n",
    "\n",
    "    return(heave, north, west)\n",
    "\n",
    "    end    # get_HNW()\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "#############################################################################################    \n",
    "# Widen screen for better viewing\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "sample_frequency = 2.56\n",
    "nyquist = sample_frequency/2\n",
    "\n",
    "# build df containing displacements and Fourier coefficient for selected day\n",
    "displacement_df = DataFrame(Time_string = [], Heave = [], North = [], West = [], Chh = [], a1 = [], b1 = [], a2 = [], b2 = [])\n",
    "\n",
    "date_string = Dates.format(start_date, \"yyyy-mm-dd HH:MM\")\n",
    "time_string = Dates.format(start_date, \"HH:MM\")\n",
    "fhh, Chh, a1, b1, a2, b2 = get_Fourier_coefficients(heave, north, west)\n",
    "\n",
    "push!(displacement_df, (time_string, heave[1:4096], north[1:4096], west[1:4096], Chh, a1, b1, a2, b2))\n",
    "    \n",
    "println(\"\\nPreparing polar plots now - this takes a while!\\n\")\n",
    "flush(stdout)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2d222",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "row = 1\n",
    "Chh = displacement_df.Chh[row]\n",
    "a1 = displacement_df.a1[row]\n",
    "b1 = displacement_df.b1[row] \n",
    "a2 = displacement_df.a2[row] \n",
    "b2 = displacement_df.b2[row]\n",
    "time_string  = displacement_df.Time_string[row]\n",
    "\n",
    "aa = length(Chh)\n",
    "\n",
    "r = 1:aa\n",
    "ρ = r ./ (aa/nyquist) \n",
    "\n",
    "θ = 1:1:361\n",
    "\n",
    "fx(r, θ) =  max(Chh[r] * (a1[r]*cos(θ) + b1[r]*sin(θ) + a2[r]*cos(2θ) + b2[r]*sin(2θ)),0)\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111, projection=\"polar\")\n",
    "\n",
    "p1 = ax.set_title(time_string)\n",
    "p1 = ax.set_theta_zero_location(\"N\")\n",
    "p1 = ax.set_theta_direction(-1)\n",
    "p1 = ax.set_ylim(0,0.4)\n",
    "\n",
    "p1 = ax.contourf(θ, ρ, fx.(r, θ'), alpha=0.95, levels=20, antialiased =:false, extend=\"both\", cmap=\"Spectral_r\")\n",
    "\n",
    "cbar = PyPlot.plt.colorbar(p1, aspect=:50, shrink=:0.75, spacing=\"proportional\", pad=:0.1)\n",
    "cbar.set_label(\"Spectral Density (m²/Hz.)\")\n",
    "\n",
    "rlabels = ax.get_ymajorticklabels()\n",
    "\n",
    "for label in rlabels\n",
    "    label.set_color(\"white\"), label.set_fontsize(10)\n",
    "end\n",
    "\n",
    "p1 = ax[:grid](:true, color=:white, ls=:dotted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58ae93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "row = 1\n",
    "Chh = displacement_df.Chh[row]\n",
    "a1 = displacement_df.a1[row]\n",
    "b1 = displacement_df.b1[row] \n",
    "a2 = displacement_df.a2[row] \n",
    "b2 = displacement_df.b2[row]\n",
    "time_string  = displacement_df.Time_string[row]\n",
    "\n",
    "aa = length(Chh)-1\n",
    "\n",
    "r = 1:aa\n",
    "ρ = r ./ (aa/nyquist) \n",
    "\n",
    "θ = 1:1:360\n",
    "\n",
    "mat =  []\n",
    "\n",
    "for j in r\n",
    "    \n",
    "    for i in θ\n",
    "    \n",
    "        push!(mat,Chh[j] * (a1[j]*cos(i) + b1[j]*sin(i) + a2[j]*cos(2i) + b2[j]*sin(2i)))\n",
    "        \n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "mat[mat .< 0] .= 0\n",
    "\n",
    "mat = reshape(mat, length(θ), length(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a21e7e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using CairoMakie\n",
    "\n",
    "fig = CairoMakie.Figure(size = (800, 800))\n",
    "    \n",
    "    ax = CairoMakie.PolarAxis(fig[1, 1],\n",
    "    title=time_string,\n",
    "    thetaticklabelsize = 12, \n",
    "    rticklabelsize = 12, \n",
    "    theta_0=-pi/2, direction=-1, \n",
    "    width=600, height=600,\n",
    "    )\n",
    "\n",
    "\n",
    "p = CairoMakie.contour!(ax, mat, linewidth = 0.1, levels=5, linestyle=:dot)\n",
    "\n",
    "##CairoMakie.Colorbar(fig[2, 1], p, vertical = false, flipaxis = false)\n",
    "\n",
    "resize_to_layout!(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ddbd84",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Dates\n",
    "using CairoMakie\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "date_time = DateTime(Date(now()))\n",
    "fig = CairoMakie.Figure(size=(1200,3000))\n",
    "supertitle = fig[0, :] = Label(fig, Dates.format(date_time, \"yyyy-mm-dd\"),\n",
    "    fontsize = 24, color = (:black, 0.25))\n",
    "\n",
    "total=-1\n",
    "\n",
    "for row = 1:8\n",
    "\n",
    "    for col in 1:6\n",
    "        \n",
    "        total+=1\n",
    "        time_string = (Dates.format(date_time + Minute(30*total), \"HH:MM\"))\n",
    "                \n",
    "        ax = CairoMakie.PolarAxis(fig[row, col],\n",
    "        title=time_string,\n",
    "        thetaticklabelsize = 12, \n",
    "        rticklabelsize = 12, \n",
    "        theta_0=-pi/2, direction=-1, \n",
    "        width=320, height=300,\n",
    "        )\n",
    "\n",
    "        CairoMakie.contour!(ax, mat, linewidth = 0.1, levels=5, linestyle=:dot)\n",
    "\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "resize_to_layout!(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039fa1b",
   "metadata": {},
   "source": [
    "## Save figure as .PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed95cb9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "CairoMakie.save(\"polar_test2.png\", fig, px_per_unit = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698ac92",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mat = [Chh[j] * (a1[j]*cos(i) + b1[j]*sin(i) + a2[j]*cos(2i) + b2[j]*sin(2i)) for i in θ, j in r]\n",
    "mat[mat.< 0] .= 0\n",
    "\n",
    "mat.<0 .= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925e388",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"Grid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28129ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "row = 1\n",
    "Chh = displacement_df.Chh[row]\n",
    "a1 = displacement_df.a1[row]\n",
    "b1 = displacement_df.b1[row] \n",
    "a2 = displacement_df.a2[row] \n",
    "b2 = displacement_df.b2[row]\n",
    "time_string  = displacement_df.Time_string[row]\n",
    "\n",
    "aa = length(Chh)\n",
    "\n",
    "r = 1:aa\n",
    "ρ = r ./ (aa/nyquist) \n",
    "\n",
    "θ = 1:1:361\n",
    "\n",
    "mat =  []\n",
    "\n",
    "for j in r\n",
    "    \n",
    "    for i in θ\n",
    "    \n",
    "        push!(mat,Chh[j] * (a1[j]*cos(i) + b1[j]*sin(i) + a2[j]*cos(2i) + b2[j]*sin(2i)))\n",
    "        \n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "mat[mat .< 0] .= 0\n",
    "\n",
    "mat = reshape(mat, length(θ), length(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f315cc7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
