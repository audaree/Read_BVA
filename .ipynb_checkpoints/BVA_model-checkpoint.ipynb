{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddea1b9-a6b0-4b71-84d6-a4289dddbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames: DataFrame, ncol, nrow\n",
    "using Dates: Dates, DateTime, Time, unix2datetime, Hour, Minute, Microsecond\n",
    "using NativeFileDialog: pick_file\n",
    "using Statistics: median, mean, std\n",
    "using Plots: Plots, plot, plot!, annotate!, hline!, @layout, text, plotly, font, scatter!\n",
    "using Printf: @sprintf\n",
    "\n",
    "#using PlotlyKaleido\n",
    "\n",
    "#plotly()\n",
    "#PlotlyKaleido.start(timeout=30)\n",
    "\n",
    "\n",
    "function get_matches(Data, f23_df)\n",
    "##################################\n",
    "    \n",
    "    # Create a dictionary to store indices of hex strings in Data\n",
    "    index_dict = Dict{String, Vector{Int}}()\n",
    "    \n",
    "    # Populate the dictionary\n",
    "    for (i, hex_str) in enumerate(Data)\n",
    "        if haskey(index_dict, hex_str)\n",
    "            push!(index_dict[hex_str], i)\n",
    "        else\n",
    "            index_dict[hex_str] = [i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Initialize a vector to store indices\n",
    "    matching_indices = []\n",
    "    \n",
    "    # Iterate through each hex string in f23_df and lookup in the dictionary\n",
    "    for hex_str in f23_df.Match_vector\n",
    "        if haskey(index_dict, hex_str)\n",
    "            push!(matching_indices, index_dict[hex_str][1])\n",
    "        else\n",
    "            push!(matching_indices, nothing)  # If no match found, store an empty vector\n",
    "        end\n",
    "    end\n",
    "\n",
    "    f23_df[!,\"Data_vector\"] = matching_indices\n",
    "\n",
    "    return(f23_df)\n",
    "\n",
    "end    # get_matches()\n",
    "\n",
    "# function to calculate selected parameters from Spectrum synchronisation message (0xF23)\n",
    "function process_f23(f23_vals)\n",
    "#######################################\n",
    "    \n",
    "    # refer to DWTP (Ver. 16 January2019) Section 4.3 pp.43-44\n",
    "\n",
    "    # get Timestamp in UTC\n",
    "    timestamp = unix2datetime(parse(Int, bitstring(f23_vals[3]) * bitstring(f23_vals[4]) * bitstring(f23_vals[5]) * bitstring(f23_vals[6]); base=2))\n",
    "    \n",
    "    # convert time to Australian Eastern Standard Time\n",
    "    timestamp = timestamp + Hour(0)  # Adjust this for the correct time zone\n",
    "\n",
    "    # get Data Stamp\n",
    "    data_stamp = parse(Int, bitstring(f23_vals[7]) * bitstring(f23_vals[8]); base=2)\n",
    "\n",
    "    # get Segments Used\n",
    "    segments_used = parse(Int, bitstring(f23_vals[9]) * bitstring(f23_vals[10]) * bitstring(f23_vals[11]); base=2)\n",
    "\n",
    "    # get Sample Number\n",
    "    sample_number = parse(Int, bitstring(f23_vals[12]) * bitstring(f23_vals[13]); base=2)\n",
    "\n",
    "    # Create Match Vector\n",
    "    match_vector = lpad(string(f23_vals[14], base=16), 2, \"0\")\n",
    "    for i in 15:22\n",
    "        match_vector = match_vector * lpad(string(f23_vals[i], base=16), 2, \"0\")\n",
    "    end\n",
    "    \n",
    "    return(timestamp, segments_used, match_vector, sample_number)\n",
    "    \n",
    "end    #  process_f23()\n",
    "\n",
    "\n",
    "# convert binary data into F23_df and Hex array\n",
    "function get_hex_array(infil)\n",
    "#############################\n",
    "    \n",
    "    # Read binary data from the input file\n",
    "    println(\"Reading BINARY data from \", infil)\n",
    "    flush(stdout)\n",
    "    data = reinterpret(UInt8, read(infil))\n",
    "    \n",
    "    # Turn the data vector into a matrix of 12 values matching hexadecimal bytes\n",
    "    cols = 12\n",
    "    rows = Int(length(data) / cols)\n",
    "    mat = reshape(view(data, :), cols, :)\n",
    "    \n",
    "    # Calculate the Heave, North, and West displacements\n",
    "    hex_matrix = string.(mat'[:,1:9], base=16, pad=2)\n",
    "    Data = [join(row) for row in eachrow(hex_matrix)]\n",
    "    \n",
    "    println(\"All file data read!\")\n",
    "    \n",
    "    # Interleave the last 3 matrix columns (10, 11, 12) to form the packet vector\n",
    "    packet = collect(Iterators.flatten(zip(mat[10,:], mat[11,:], mat[12,:])))\n",
    "    \n",
    "    # Find all occurrences of 0x7e in the packet vector\n",
    "    aa = findall(x -> x == 0x7e, vec(packet))\n",
    "    \n",
    "    # Create DataFrame to hold the processed data\n",
    "    f23_df = DataFrame(Date = DateTime[], Segments = Int[], Match_vector = String[], Sample_number = Int[])\n",
    "    \n",
    "    # Decode the packet data into messages\n",
    "    max_val = length(aa) - 1\n",
    "    \n",
    "    for i in 1:max_val\n",
    "        first = aa[i] + 1\n",
    "        last = aa[i + 1]\n",
    "        \n",
    "        if (last - first > 1)\n",
    "            decoded = packet[first:last-1]\n",
    "            \n",
    "            # Handle the 0x7d escape sequences (XOR with 0x20)\n",
    "            bb = findall(x -> x == 0x7d, decoded)\n",
    "            for ii in bb\n",
    "                decoded[ii + 1] = decoded[ii + 1] ‚äª 0x20\n",
    "            end\n",
    "            deleteat!(decoded, bb)\n",
    "            \n",
    "            # If the message is F23 (0x23)\n",
    "            if decoded[2] == 0x23\n",
    "                timestamp, segments_used, match_vector, sample_number = process_f23(decoded)\n",
    "                push!(f23_df, [timestamp, segments_used, match_vector, sample_number])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Remove duplicates from f23_df\n",
    "    f23_df = unique(f23_df);\n",
    "\n",
    "    return(f23_df, Data)\n",
    "    \n",
    "    end    # get_hex_array()\n",
    "\n",
    "\n",
    "function get_start_end_dates(f23_df,found_list)\n",
    "###############################################\n",
    "    \n",
    "    start_date = f23_df[found_list[1],:].Date - Minute(30) # <------- NOTE subtracted 30min from start_date to match Waves4 results\n",
    "    segments = f23_df[found_list[1],:].Segments\n",
    "#   match_vector = f23_df[found_list[1],:].Match_vector\n",
    "    sample_nos = f23_df[found_list[1],:].Sample_number\n",
    "    data_vector = f23_df[found_list[1],:].Data_vector\n",
    "    start_val = data_vector - Int(sample_nos/2) + 1\n",
    "    end_val = data_vector\n",
    "    \n",
    "    return(start_date,start_val, end_val)\n",
    "    \n",
    "end    #(get_start_end_dates)\n",
    "    \n",
    "  \n",
    "function get_displacement(Data, start_val, end_val)\n",
    "################################################\n",
    "# Decode the real time data to displacements - See DWTP (16 Jan 2019) 2.1.1 p. 19    \n",
    "    \n",
    "    arry = collect(Iterators.flatten(zip(SubString.(Data, start_val, end_val),SubString.(Data, start_val+9, end_val+9))));\n",
    "    displacements = [parse(Int, SubString.(i, 1, 1), base=16)*16^2 + parse(Int, SubString.(i, 2, 2), base=16)*16^1 + parse(Int, SubString.(i, 3, 3), base=16)*16^0 for i in arry]    \n",
    "    \n",
    "    displacements[findall(>=(2048), displacements)] = displacements[findall(>=(2048), displacements)] .- 4096;\n",
    "    displacements = 0.457*sinh.(displacements/457)    # see DWTP p.19 (16)\n",
    "   \n",
    "    return(displacements)\n",
    "    \n",
    "end    # get_displacement()\n",
    "\n",
    "\n",
    "function get_hnw(Data,start_val,end_val)\n",
    "######################################## \n",
    "    \n",
    "    # get WSEs for desired 30-minute record\n",
    "    heave = get_displacement(Data[start_val:end_val,:], 1, 3);              \n",
    "    north = get_displacement(Data[start_val:end_val,:], 4, 6);\n",
    "    west = get_displacement(Data[start_val:end_val,:], 7, 9);\n",
    "    \n",
    "    # Check for missing or extra points in data\n",
    "    for wse in [heave, north, west]\n",
    "        \n",
    "        wse_length = length(wse)\n",
    "        \n",
    "        if wse_length > REC_LENGTH\n",
    "\n",
    "            # truncate if too long\n",
    "            wse = wse[1:REC_LENGTH]\n",
    "            \n",
    "        else\n",
    "\n",
    "            # zero pad if too short (leave it unchanged if right length)\n",
    "            append!(wse,zeros(REC_LENGTH-wse_length))\n",
    "            \n",
    "        end      \n",
    "\n",
    "    end\n",
    "    \n",
    "    return (heave, north, west)\n",
    "    \n",
    "end    # get_hnw()\n",
    "\n",
    "\n",
    "# Function to calculate confidence limits\n",
    "function calc_confidence_limits(data, confidence_interval)\n",
    "##########################################################\n",
    "    \n",
    "    mean_val = mean(data)\n",
    "    std_dev = std(data)\n",
    "    upper_limit = mean_val + confidence_interval * std_dev\n",
    "    lower_limit = mean_val - confidence_interval * std_dev\n",
    "    \n",
    "    return (lower_limit, upper_limit)\n",
    "    \n",
    "end    # calc_confidence_limits()\n",
    "\n",
    "\n",
    "# Function to compute modified z-scores and find outliers\n",
    "function modified_z_score(data, threshold)\n",
    "##########################################\n",
    "    \n",
    "    med = median(data)\n",
    "    mad = median(abs.(data .- med))\n",
    "    mod_z_scores = 0.6745 * (data .- med) ./ mad\n",
    "    outlier_indices = findall(x -> abs(x) > threshold, mod_z_scores)\n",
    "    \n",
    "    return(outlier_indices, mod_z_scores)\n",
    "    \n",
    "end    # modified_z_score()\n",
    "\n",
    "\n",
    "# Function for dynamic threshold based on mean wave height\n",
    "function dynamic_z_score_threshold(heave, base_threshold=3.0, k=0.5)\n",
    "    \n",
    "    mean_wave_height = mean(heave)\n",
    "    std_wave_height = std(heave)\n",
    "    dynamic_threshold = base_threshold * (1 + k * (mean_wave_height / std_wave_height))\n",
    "    \n",
    "    return(dynamic_threshold)\n",
    "    \n",
    "end    # dynamic_z_score_threshold()\n",
    "\n",
    "\n",
    "function pad_or_truncate(record, target_length=REC_LENGTH)\n",
    "####################################################\n",
    "\n",
    "    length(record) < target_length ? vcat(record, zeros(Float32, target_length - length(record))) :\n",
    "                                     record[1:target_length]\n",
    "\n",
    "end    # pad_or_truncate()\n",
    "\n",
    "\n",
    "function get_heave(Data, f23_df)\n",
    "################################\n",
    "    \n",
    "    heave_array = []\n",
    "    X_date = []\n",
    "\n",
    "    println(\"Calculating Heave values now!\")\n",
    "    \n",
    "    for idx in 1:nrow(f23_df)\n",
    "\n",
    "        if !isnothing(f23_df.Data_vector[idx])\n",
    "    \n",
    "            start_date, start_val, end_val = get_start_end_dates(f23_df,idx)\n",
    "            if start_val > 0\n",
    "                print(\".\")\n",
    "                heave, north, west = get_hnw(Data,start_val,end_val)\n",
    "\n",
    "                # ensure we have REC_LENGTH data points\n",
    "                push!(heave_array,pad_or_truncate(heave, REC_LENGTH))\n",
    "                push!(X_date,start_date)\n",
    "            end\n",
    "\n",
    "        end\n",
    "    \n",
    "    end\n",
    "\n",
    "    return(hcat(heave_array...), X_date)\n",
    "\n",
    "end    # get_heave()\n",
    "\n",
    "\n",
    "# Need to check first row of the f23_df in case 23:00 is stored there\n",
    "function f23_first_row_check(f23_df)\n",
    "################################\n",
    "    \n",
    "    # Get the first row of the DataFrame\n",
    "    first_row = first(f23_df)\n",
    "    \n",
    "    # Check if the time of the first row's Date column is 23:00:00\n",
    "    time_of_first_row = Time(first_row.Date)\n",
    "\n",
    "    if time_of_first_row == Time(23, 0, 0)\n",
    "\n",
    "        if ismissing(first_row.Data_vector) || isnothing(first_row.Data_vector) || isnan(first_row.Data_vector)\n",
    "            f23_df = f23_df[2:end, :]  # Drop the first row\n",
    "        end\n",
    "\n",
    "    end\n",
    "    \n",
    "    return(f23_df)\n",
    "    \n",
    "    end    # f23_first_row_check()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "const REC_LENGTH = 4608\n",
    "const SAMPLE_FREQUENCY = 2.56 # sample frequency in Hertz\n",
    "const SAMPLE_LENGTH = 1800 # record length in seconds\n",
    "const SAMPLE_RATE = Float64(1/SAMPLE_FREQUENCY) # sample spacing in seconds\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))\n",
    "\n",
    "infil = pick_file()\n",
    "\n",
    "f23_df, Data = get_hex_array(infil)\n",
    "\n",
    "f23_df = get_matches(Data, f23_df)\n",
    "\n",
    "# remove those vectors from F23 df that are not located in the Data vector df\n",
    "f23_df = f23_first_row_check(f23_df)\n",
    "\n",
    "X_train, X_date = get_heave(Data, f23_df);\n",
    "\n",
    "println(\"\\nNow preparing to plot heave for each record!\")\n",
    "flush(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34574024-756c-424d-aae7-f3a88fd46a9b",
   "metadata": {},
   "source": [
    "### Plot each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc1e9b-777a-472d-8a26-891ee2359971",
   "metadata": {},
   "outputs": [],
   "source": [
    "##==\n",
    "# Loop through wave records\n",
    "for ii in 1:10 #length(X_date)\n",
    "    \n",
    "    # Initialize variables\n",
    "    start_time = X_date[ii]\n",
    "    heave = X_train[:, ii]\n",
    "    end_time = start_time + Minute(30)\n",
    "    xvals = start_time + Microsecond.((0:REC_LENGTH-1) / SAMPLE_FREQUENCY * 1000000)\n",
    "\n",
    "    # Plot initialization\n",
    "    p1 = plot(size=(1200, 300), dpi=100, framestyle=:box, fg_legend=:transparent, bg_legend=:transparent, \n",
    "        legend=:topright, xtickfont=font(8), ytickfont=font(8),\n",
    "        grid=true, gridlinewidth=0.125, gridstyle=:dot, gridalpha=1)\n",
    "    \n",
    "    tm_tick = range(start_time, end_time, step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick, \"MM\")\n",
    "    \n",
    "    # Calculate dynamic confidence interval\n",
    "    confidence_interval = dynamic_z_score_threshold(heave)\n",
    "\n",
    "    # Identify outliers using modified z-score\n",
    "    outlier_indices, mod_z_scores = modified_z_score(heave, confidence_interval)\n",
    "    if !isempty(outlier_indices)\n",
    "        scatter!(p1, xvals[outlier_indices], heave[outlier_indices], \n",
    "            markersize=4, markerstrokecolor=:red, markerstrokewidth=1, \n",
    "            markercolor=:white, markershape=:circle, label=\"\")\n",
    "    end\n",
    "\n",
    "    # Plot confidence limits\n",
    "    confidence_limits = calc_confidence_limits(heave, confidence_interval)\n",
    "    hline!(p1, [confidence_limits[1], confidence_limits[2]], color=:red, lw=0.5, linestyle=:dash, label=\"\")\n",
    "\n",
    "    # Plot heave data\n",
    "    plot!(p1, xvals, heave, xlims=(xvals[1], xvals[end]), lw=0.5, lc=:blue, alpha=0.5, \n",
    "        xticks=(tm_tick, ticks), label=\"\")\n",
    "\n",
    "    # Annotate plot with the number of outliers and confidence interval\n",
    "    num_outliers = length(outlier_indices)\n",
    "    suspect_string = string(\"  \", Dates.format(start_time, \"yyyy-mm-dd HH:MM\"), \" - \", num_outliers, \" Possible outliers using Confidence Interval of \", \n",
    "        @sprintf(\"%.2f\", confidence_interval))\n",
    "    annotate!(p1, xvals[1], maximum(heave) * 0.9, text(suspect_string, :left, 10, :blue))\n",
    "\n",
    "    display(p1)\n",
    "\n",
    "end\n",
    "#==#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ac93d-fe87-45cc-a7e3-e37ea04318ac",
   "metadata": {},
   "source": [
    "### Select records that will NOT be uploaded to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385b693-82e8-47cb-96e1-7ef543688d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Tk\n",
    "\n",
    "all_dates = Dates.format.(X_date, \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "w = Toplevel(\"Select Date\", 235, 400)\n",
    "tcl(\"pack\", \"propagate\", w, false)\n",
    "f = Frame(w)\n",
    "pack(f, expand=true, fill=\"both\")\n",
    "\n",
    "f1 = Frame(f)\n",
    "lb = Treeview(f1, all_dates)\n",
    "scrollbars_add(f1, lb)\n",
    "pack(f1,  expand=true, fill=\"both\")\n",
    "\n",
    "tcl(\"ttk::style\", \"configure\", \"TButton\", foreground=\"blue\", font=\"arial 16 bold\")\n",
    "b = Button(f, \"Ok\")\n",
    "pack(b)\n",
    "\n",
    "bad_array = []\n",
    "\n",
    "bind(b, \"command\") do path\n",
    "    \n",
    "    file_choice = get_value(lb);\n",
    "    push!(bad_array,file_choice[1])\n",
    "\n",
    "end\n",
    "\n",
    "# Find indices of bad_dates in X_date\n",
    "bad_cols = findall(x -> x in bad_array, all_dates)\n",
    "\n",
    "# Remove columns from X_train whose column numbers are in bad_cols\n",
    "X_train = X_train[:, setdiff(1:size(X_train, 2), bad_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422400d6-e8d6-4d13-a96d-ea4896b02596",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Tk\n",
    "\n",
    "# Convert DateTimes into strings\n",
    "all_dates = Dates.format.(X_date, \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "# Initialize selection window\n",
    "w = Toplevel(\"Select Date\", 235, 400)\n",
    "tcl(\"pack\", \"propagate\", w, false)\n",
    "f = Frame(w)\n",
    "pack(f, expand=true, fill=\"both\")\n",
    "\n",
    "f1 = Frame(f)\n",
    "lb = Treeview(f1, all_dates)\n",
    "scrollbars_add(f1, lb)\n",
    "pack(f1, expand=true, fill=\"both\")\n",
    "\n",
    "# Style button\n",
    "tcl(\"ttk::style\", \"configure\", \"TButton\", foreground=\"blue\", font=\"arial 16 bold\")\n",
    "b = Button(f, \"Ok\")\n",
    "pack(b)\n",
    "\n",
    "# Global array to store selected dates\n",
    "global bad_array = []\n",
    "\n",
    "# Collect dates when the button is pressed\n",
    "bind(b, \"command\") do path\n",
    "    file_choice = get_value(lb)\n",
    "    if !isempty(file_choice)\n",
    "        push!(bad_array, file_choice[1])\n",
    "        println(\"Added to removal list: \", file_choice[1])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Function to close window and process columns\n",
    "function finalize_selection(args...)\n",
    "    destroy(w)  # Close Tk window\n",
    "\n",
    "    # Find indices of bad_dates in X_date\n",
    "    bad_cols = findall(x -> x in bad_array, all_dates)\n",
    "\n",
    "    # Remove columns from X_train based on `bad_cols`\n",
    "    global X_train = X_train[:, setdiff(1:size(X_train, 2), bad_cols)]\n",
    "    println(\"Removed columns based on selection.\")\n",
    "end\n",
    "\n",
    "# Bind finalize function to window close event\n",
    "tcl(\"wm\", \"protocol\", w, \"WM_DELETE_WINDOW\", finalize_selection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4664893-b118-481a-93fb-59dbdd909928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train is your matrix\n",
    "first_5_cols = X_train[:, 1:5]  # Get the first 5 columns\n",
    "last_5_cols = X_train[:, end-4:end]  # Get the last 5 columns\n",
    "\n",
    "# Combine them into a new matrix for display\n",
    "combined = hcat(first_5_cols, last_5_cols)  # Horizontally concatenate\n",
    "\n",
    "println(combined)  # Display the combined matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f88a9-21c8-43cb-9ed8-dc60e1456893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train is your matrix\n",
    "first_5_rows = X_train[1:5, :]  # Get the first 5 rows\n",
    "last_5_rows = X_train[end-4:end, :]  # Get the last 5 rows\n",
    "\n",
    "first_5_cols = X_train[:, 1:5]  # Get the first 5 columns\n",
    "last_5_cols = X_train[:, end-4:end]  # Get the last 5 columns\n",
    "\n",
    "# Combine the first and last 5 rows and columns into a new matrix for display\n",
    "combined = hcat(first_5_cols, last_5_cols)  # Horizontally concatenate first and last columns\n",
    "display = vcat(first_5_rows, last_5_rows)  # Vertically concatenate first and last rows\n",
    "\n",
    "# Now you can print the combined results\n",
    "println(\"First 5 rows:\")\n",
    "println(first_5_rows)\n",
    "\n",
    "println(\"\\nLast 5 rows:\")\n",
    "println(last_5_rows)\n",
    "\n",
    "println(\"\\nCombined display (first and last 5 columns):\")\n",
    "println(display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19655153-0295-4b8e-866c-d0027b19a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e6424-cd11-4bbd-b628-ddd62db0e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087b671-2d01-4048-b0fd-78be2e3489de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_old = X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec68f3f1-be32-4646-8744-6b42c5672f28",
   "metadata": {},
   "source": [
    "### Initial Code for Training an Autoencoder in Julia (using Flux.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c254d-570b-41bf-b65d-37c15f4d48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "\n",
    "function min_max_normalize_matrix(X)\n",
    "    min_vals = minimum(X, dims=1)  # Compute min for each column\n",
    "    max_vals = maximum(X, dims=1)  # Compute max for each column\n",
    "    return (X .- min_vals) ./ (max_vals .- min_vals)\n",
    "end\n",
    "\n",
    "\n",
    "function z_score_normalize_matrix(X)\n",
    "    mean_vals = mean(X, dims=1)  # Mean for each column\n",
    "    std_vals = std(X, dims=1)    # Standard deviation for each column\n",
    "    return (X .- mean_vals) ./ std_vals\n",
    "end\n",
    "\n",
    "\n",
    "function pad_or_truncate(record, target_length=4608)\n",
    "####################################################\n",
    "#==    \n",
    "    if length(record) < target_length\n",
    "        # Pad with zeros (or any other value you prefer)\n",
    "        return vcat(record, zeros(Float32, target_length - length(record)))\n",
    "    elseif length(record) > target_length\n",
    "        # Truncate to the target length\n",
    "        return record[1:target_length]\n",
    "    else\n",
    "        return record\n",
    "    end\n",
    "==#\n",
    "    length(record) < target_length ? vcat(record, zeros(Float32, target_length - length(record))) :\n",
    "                                     record[1:target_length]\n",
    "\n",
    "end    # pad_or_truncate()\n",
    "\n",
    "\n",
    "function get_heave(Data, f23_df)\n",
    "################################\n",
    "    \n",
    "    heave_array = []\n",
    "    X_date = []\n",
    "    \n",
    "    for idx in 1:nrow(f23_df)\n",
    "\n",
    "        if !isnothing(f23_df.Data_vector[idx])\n",
    "    \n",
    "            start_date, start_val, end_val = get_start_end_dates(f23_df,idx)\n",
    "            if start_val > 0\n",
    "                print(\".\")\n",
    "                heave, north, west = get_hnw(Data,start_val,end_val)\n",
    "\n",
    "                # ensure we have 4608 data points\n",
    "                push!(heave_array,pad_or_truncate(heave, 4608))\n",
    "                push!(X_date,start_date)\n",
    "            end\n",
    "\n",
    "        end\n",
    "    \n",
    "    end\n",
    "\n",
    "    return(hcat(heave_array...), X_date)\n",
    "\n",
    "end    # get_heave()\n",
    "\n",
    "\n",
    "function calc_reconstruction_errors(X_train_float32, model)\n",
    "####################################################\n",
    "    \n",
    "    reconstruction_errors = Float32[]\n",
    "    \n",
    "    for record in eachcol(X_train_float32)  # Each record is now a column with 14 features\n",
    "        reconstructed_record = model(record)  # Pass the record to the autoencoder\n",
    "        error = mean((reconstructed_record .- record).^2)  # Calculate the reconstruction error\n",
    "        push!(reconstruction_errors, error)  # Store the error\n",
    "    end\n",
    "    \n",
    "    return(reconstruction_errors)\n",
    "\n",
    "end    # calc_reconstruction_errors()\n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "@time begin\n",
    "# Define autoencoder model\n",
    "model = Chain(\n",
    "    Dense(4608, 128, relu),  # Encoder\n",
    "    Dense(128, 64, relu),    # Bottleneck\n",
    "    Dense(64, 128, relu),    # Decoder\n",
    "    Dense(128, 4608)         # Output layer, reconstructs input\n",
    ")\n",
    "\n",
    "# Define the loss function (e.g., Mean Squared Error for reconstruction)\n",
    "loss(x) = Flux.mse(model(x), x)\n",
    "\n",
    "# Optimizer: Adam with default parameters (learning rate, etc.)\n",
    "opt = Adam()\n",
    "   \n",
    "X_train, X_date = get_heave(Data, f23_df)\n",
    "\n",
    "# Normalize your training data\n",
    "##X_train_normalized = normalize_records(X_train)\n",
    "X_train_normalized = min_max_normalize_matrix(X_train)\n",
    "    \n",
    "# Convert WSE data to Float32\n",
    "X_train_float32 = Float32.(X_train_normalized)\n",
    "\n",
    "# calculate the reconstruction_errors\n",
    "reconstruction_errors_model = calc_reconstruction_errors(X_train_float32, model)\n",
    "\n",
    "# Use the converted data for training\n",
    "data = Iterators.repeated((X_train_float32,), 100)  # Example of data iteration for training\n",
    "\n",
    "# Train the model\n",
    "Flux.train!(loss, Flux.params(model), data, opt)\n",
    "end    # @time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d360a-43d7-4030-b6c2-ad4657538b1b",
   "metadata": {},
   "source": [
    "### Save model, optimiser, and normalised heave data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e3f1c-d875-486f-97b6-3a1098d00c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, JLD2\n",
    "\n",
    "# Save the model and optimizer\n",
    "model_state = Flux.state(model)\n",
    "opt_state = opt # Flux.setup(Adam(), model)\n",
    "\n",
    "outfil = \"HVA_model_\"*Dates.format(now(), \"yyyy_mm_dd_HHMM\")*\".jld2\" \n",
    "\n",
    "# Save model and optimizer and normalised wave data to a JLD2 file\n",
    "jldsave(outfil; model_state, opt_state, X_train_float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7aeb7-d80a-4270-88b7-27189c909369",
   "metadata": {},
   "source": [
    "### Recover saved model, optimiser, and normalised heave data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e3a17-8520-4168-8b41-bba92c41515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2, Flux\n",
    "\n",
    "# Load the model and optimizer states from the JLD2 file\n",
    "infil = pick_file()\n",
    "loaded_data = jldopen(infil, \"r\") do file\n",
    "    old_model_state = file[\"model_state\"]  # Load model state\n",
    "    old_opt_state = file[\"opt_state\"]      # Load optimizer state\n",
    "    old_X_train_float32 = file[\"X_train_float32\"]  # Load the previous X_train_float32 data\n",
    "    return (old_model_state, old_opt_state, old_X_train_float32) # Return all states and data\n",
    "end\n",
    "\n",
    "old_model_state, old_opt_state, old_X_train_float32 = loaded_data\n",
    "\n",
    "# Define the old model architecture\n",
    "old_model = Chain(\n",
    "    Dense(4608, 128, relu),  # Encoder\n",
    "    Dense(128, 64, relu),    # Bottleneck\n",
    "    Dense(64, 128, relu),     # Decoder\n",
    "    Dense(128, 4608)          # Output layer, reconstructs input\n",
    ")\n",
    "\n",
    "# Restore model parameters from the loaded state\n",
    "for (layer, state) in zip(old_model.layers, old_model_state[:layers])\n",
    "    layer.weight .= state.weight   # Assign saved weights\n",
    "    layer.bias .= state.bias       # Assign saved biases\n",
    "end\n",
    "\n",
    "# Restore the optimizer state directly\n",
    "old_opt = old_opt_state;  # Assign the loaded optimizer state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f8c1af-5657-4569-80d4-d603a52c45a0",
   "metadata": {},
   "source": [
    "### Append new data to existing model and retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddcb9dd-3844-43d9-980e-02fb6c01d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using JLD2\n",
    "\n",
    "infil = pick_file()\n",
    "\n",
    "# Load the existing model and optimizer\n",
    "loaded_model, opt = JLD2.@load infil model opt\n",
    "\n",
    "# Load new records and prepare the data\n",
    "new_X_train, _ = get_heave(Data, f23_df)  # Replace with your new data fetching function\n",
    "new_X_train_normalized = normalize_records(new_X_train)\n",
    "new_X_train_float32 = Float32.(new_X_train_normalized)\n",
    "\n",
    "# Combine with the previous training data (if applicable)\n",
    "# You can concatenate with previous training data if desired\n",
    "X_combined = hcat(old_X_train_float32, new_X_train_float32)\n",
    "\n",
    "# Define the loss function again\n",
    "loss(x) = Flux.mse(loaded_model(x), x)\n",
    "\n",
    "# Prepare the data for training (iterating over the new combined data)\n",
    "data = Iterators.repeated((X_combined,), 100)\n",
    "\n",
    "# Train the model on the new data\n",
    "Flux.train!(loss, Flux.params(loaded_model), data, opt)\n",
    "\n",
    "outfil = infil\n",
    "\n",
    "# Save model and optimizer and normalised wave data to a JLD2 file\n",
    "jldsave(outfil; model_state, opt_state, X_train_float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a49f83-effa-4fe5-8d4a-f3c91a436a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51606fd2-eb20-4113-a3be-0f7f6e716b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
